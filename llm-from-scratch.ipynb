{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MathObsession/AI-voice-assistant/blob/main/llm-from-scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CzlhwVdbxKb7"
      },
      "source": [
        "# Building a Simple GPT-Style Q&A LLM from Scratch\n",
        "\n",
        "*A good resource to use alongside this notebook is the original [GPT paper](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf) [1]. This notebook largely relies on that paper for model architectures and implementation.*\n",
        "\n",
        "\n",
        "This article will walk through building a simple GPT style model from scratch using pytorch [1,2]. The goal of this article is to train a basic large language model from start to finish in one notebook. We will train an LLM that is small enough to fit in a single GPU during training and inference, so the notebook can be run in popular cloud GPU services (Google Colab, Kaggle, Paperspace, etc...). The computation graph of the model that we will build in this article is as follows:\n",
        "\n",
        "<div style=\"width:500px;margin:auto;\">\n",
        "\n",
        "![image-2.png](attachment:image-2.png)\n",
        "\n",
        "</div>\n",
        "\n",
        "This architecture resembles the original [GPT](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf) model, and is quite similar to [GPT2](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) and [GPT3](https://arxiv.org/pdf/2005.14165), with the main difference being that it is smaller (less decoder blocks and smaller embedding sizes) [1,3,4]. We will zoom into each step of this diagram throughout this article to discuss the math, code, and intuition behind them.\n",
        "\n",
        "According to the original GPT paper, there are two main training stages for the early GPT models, **pretraining** and **supervised fine tuning** [1]. Pretraining is a self supervised learning task, where parts of the input data are omitted and used as target variables. Self supervised fine tuning works similar to traditional supervised learning tasks, with human annoted labels for input data.    \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iryduj-3xKb8"
      },
      "source": [
        "# 1: Pretraining   \n",
        "The first stage in building a GPT model is pretraining. Pretraining builds the \"base\" of an LLM. It allows the model to understand statistical properties of language, grammar, and context.\n",
        "\n",
        "#### Pretraining Goal\n",
        "\n",
        "The goal of pretraining is simple: **to have a model that can reliably predict the next token given the previous k tokens in a sequence**. The final result of pretraining is a deep learning model that takes in $k$ tokens and produces a discrete probability distribution of what the $k+1$ token should be. We want this distribution to show a high value for the correct token and low values for the incorrect ones.\n",
        "\n",
        "<div style=\"width:600px;margin:auto;\">\n",
        "\n",
        "![image-2.png](attachment:image-2.png)\n",
        "\n",
        "</div>\n",
        "\n",
        "\n",
        "To achieve this, we start off with a large dataset of raw text. This text can be taken from books, blogs, wikis, research papers, and other text sources. After compiling the large dataset of text, we split the dataset into \"chunks\" of tokens, where each chunk has a certain amount of tokens (512 gpt, 1024 gpt2, 16385 gpt-3). This chunk size is known as the \"context window\". A pretrained model will take in that many tokens, and output the most likely next token.\n",
        "\n",
        "#### What is a Token?\n",
        "When dealing with LLMs we use the word \"token\" to describe the smallest \"unit\" of text that an LLM can analyze [5]. Tokens can generally be thought of as words conceptually. When analyzing a sequence of text, an LLM first has to convert the text to tokens. This is similar to a dictionary lookup, each word/token will have an integer \"index\" in the lookup. This index is what will actually be fed into the network to be analyzed.\n",
        "\n",
        "<div style=\"width:600px;margin:auto;\">\n",
        "\n",
        "![image-3.png](attachment:image-3.png)\n",
        "\n",
        "</div>\n",
        "\n",
        "#### Pretraining Data Format\n",
        "Each example of the pretraining dataset is a chunk of tokens. The same chunk of tokens is used for the input and output, but the output is shifted 1 token into the \"future\". The reason for this has to do with the parallel processing capabilities of the transformer, which we will go into depth further in the transformer section. The following visual helps show what the training data looks like for the pretraining model.    \n",
        "\n",
        "<div style=\"width:600px;margin:auto;\">\n",
        "\n",
        "![image.png](attachment:image.png)   \n",
        "\n",
        "</div>\n",
        "\n",
        "Because the model uses transformers and parallel processing, a single example like the one above is actually in a sense 6 different examples. The model is learning the following predictive patterns:   \n",
        "- When input = in, output = the\n",
        "- When input = in the, output = morning\n",
        "- When input = in the morning, output = the\n",
        "- When input = in the morning the, output = sky\n",
        "- When input = in the morning the sky, output = is\n",
        "- when input = in the morning, the sky is, output = blue\n",
        "\n",
        "This will be clearer in the transformer section of the article. The main point to know now is what the format of the input and outputs of the training data should look like in the pretraining step. The outputs are the inputs, shifted by one token so that each input token aligns with the output token that comes directly after it in the original sequence.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U9sv9fYoxKb9"
      },
      "source": [
        "## 1.1: Download Pretraining Dataset\n",
        "\n",
        "Before doing a full pre-training loop, we will do a \"test run\" using a small dataset we can fit in to memory. This will allow us to focus on the internals of the model rather than complexities of data processing. We can use the [Salesforce wikitext](https://huggingface.co/datasets/EleutherAI/wikitext_document_level) dataset that consists of an extract of good and featured wikipedia articles [6].   \n",
        "\n",
        "We will load the dataset from the [huggingface datasets hub](https://huggingface.co/docs/datasets/en/load_hub). The huggingface datasets package provides an easy way to load, preprocess, and use a variety of datasets for deep learning [7]."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "R7utlD8IxKb9",
        "outputId": "910dd701-264a-4abc-bea4-cefd1aa708f2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "Collecting datasets\n",
            "  Downloading datasets-4.4.2-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets) (3.20.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.0.2)\n",
            "Collecting pyarrow>=21.0.0 (from datasets)\n",
            "  Downloading pyarrow-22.0.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.32.4)\n",
            "Requirement already satisfied: httpx<1.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.28.1)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.19 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.10.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub<2.0,>=0.25.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.36.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets) (6.0.3)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.13.2)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (4.12.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (2025.11.12)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.22.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading datasets-4.4.2-py3-none-any.whl (512 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m512.3/512.3 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyarrow-22.0.0-cp312-cp312-manylinux_2_28_x86_64.whl (47.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.7/47.7 MB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyarrow, datasets\n",
            "  Attempting uninstall: pyarrow\n",
            "    Found existing installation: pyarrow 18.1.0\n",
            "    Uninstalling pyarrow-18.1.0:\n",
            "      Successfully uninstalled pyarrow-18.1.0\n",
            "  Attempting uninstall: datasets\n",
            "    Found existing installation: datasets 4.0.0\n",
            "    Uninstalling datasets-4.0.0:\n",
            "      Successfully uninstalled datasets-4.0.0\n",
            "Successfully installed datasets-4.4.2 pyarrow-22.0.0\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.12/dist-packages (0.12.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken) (2025.11.3)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.12/dist-packages (from tiktoken) (2.32.4)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (2025.11.12)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.11.12)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.5.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.61.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "! pip install --upgrade datasets\n",
        "! pip install tiktoken\n",
        "! pip install transformers\n",
        "! pip install torch\n",
        "! pip install matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "MWXdr9GHxKb9",
        "outputId": "2597692d-cf58-42f7-a7e2-90fdb835f255",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "import warnings\n",
        "import torch\n",
        "import math\n",
        "import time\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from itertools import cycle\n",
        "from datasets import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import AutoTokenizer\n",
        "from torch.optim.lr_scheduler import _LRScheduler\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "CIsrbN4AxKb-",
        "outputId": "f333ecdb-e85d-4793-ec31-1f2ce8b26ad9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 261,
          "referenced_widgets": [
            "676f0a9df47e4b1780a1719ee9a36799",
            "2b1f827611334219a87b2bf4eeb125fa",
            "5c82932ed45541fa8874a155d7ef8bc0",
            "acada8fa12fc490c8144cb58e5eff5e5",
            "7608de0d7776416da8ead1a6571f1d81",
            "f5ddd2be42a04a4ca081d6d7e7f7e0aa",
            "78dd831a2b1e4771b231c3b3ce9bb8b0",
            "dfd17a3ba21342c5a80af27c6b02f014",
            "dd7289f68b2641f0a3819c85028bb579",
            "86f8fbea683b4fa196af95f94e475ed1",
            "6daa4bc31aa24c6c822a93d85421dad9",
            "54f8bbdf3f1340d58ce36198fb0f571e",
            "9c731d95857a4a73a571904f752561da",
            "c655b5dc24614a60b92cff86547aa011",
            "489656a9fbb04a528dd581792483fae9",
            "f1fe334d21b84df9a69fe61c110376be",
            "badf75616ce0469496bb8b484a90ef81",
            "36648846a61c426ba7b214b0cf19df11",
            "17c3de396aa840628078ff3beaf8b152",
            "abe31d274e7141f7b83a9ba9d3b798ca",
            "474449ab477740deb0bdafb78550bfde",
            "ca4887bd7aaa43db92e1e4bd2ecd5b1a",
            "dd7411cdd0fe46e9937b4bad0a38ee00",
            "90d253e05204480ca5ecfc28b31efcc1",
            "522b5fba838a4fa89e87b7fb9b33755c",
            "6e1bd04e8cf14915bb84707a45931631",
            "d1265802ef7e4b0aaf1c33802e38f4b0",
            "8c2bb652770e4b9d9005a1bbb114c045",
            "2d14eb1733784154844b120585c4f8cc",
            "240c2a477007430fa86ca2a9cbdc90e7",
            "c42a0a3eee244624b1eb18307c4e6239",
            "fa84b09d70024f9cab9befbc7ef690c3",
            "bf9e86f673454ba088f0b7bdccb8006b",
            "364dd88fa9304a28900876206f3dd279",
            "751fd407ed8643a89f067f5d3c8320bd",
            "48334931bf9142f089ae44714a1c6ec8",
            "6270c3c069c74627be1ed565c529ebef",
            "83cca88897474b98800a582060c60515",
            "5eee864a2963411b861119fb86c48df0",
            "32db1c53ae7f4f81864f6dc26ec94039",
            "411ae67af1ba46a4ad6966152a596408",
            "7148ca36f85648969134e2960885ef59",
            "ac4d02661d8d444eb5e4495046307bf5",
            "6e017ce6e7534b92b1ce79906c9a4973",
            "49a97eeb72a64afe9e7dbbb9190ee0f4",
            "aec33ef6544b447fb2fdaf31dbc62250",
            "cd3b7378291b48338ee6104aea96e775",
            "32f29fc62c444b7fb35e35d749fcd297",
            "6a4d2d81c2d64092b71a94c21be271e3",
            "c1f5c05bcfd140899fed3461b32450ae",
            "25feec3f13224bd4b3e84d753a2cabe4",
            "f5e97813d236455daea8e2295d9cbe7c",
            "7bb7bfb6d04e4af5b8668634f26df806",
            "6f9acb0deafc42a2a90730db002cb884",
            "eb071085bafa49f68d542f811fe2f360",
            "6a2f0255c0c74a7b9ad90853d1f5aa2f",
            "64060c0957314d8184fde547c2f7674c",
            "fe2b9d1b64e44e5295e810c221fc9b79",
            "efa4c0c93fd54c8783c43d0453175acf",
            "575b0ed3c76b428f94ad1354168cf71b",
            "5d1f07008db74fdcb1c9c07dc0187254",
            "a623c7f65a2e4bf499f5b77960c399d4",
            "5269ba94a10d40bcb00cfba2a9a28667",
            "16326b01882447ada08b093974d7c069",
            "c925a75f9a1e4e3a8859c07ed934442b",
            "9d94fa39c5f64734a9f45ca909e4ec52",
            "df3c8500ad7f4468815648693de092d2",
            "461e81974b4442f8a10f79590f6b7eb7",
            "22733d11590743d5b751c8b828b9a879",
            "60f4c2ff247d499d9018cd231c7dc7c3",
            "21e1a2d783734364b1bff3c736f1e115",
            "b8db1efa578f478e8d85229266b12bfa",
            "e0d8c50e8e9149ae8fb17a106ac9d4ff",
            "58ab936e783147659591d07a51dd993b",
            "44bedb1cfdc04a6fa95d0802cf223b0c",
            "fb64a20bc4e14f9a8150f136ce87ee75",
            "d2c981a66ac241aeafee87639a769b6f"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "676f0a9df47e4b1780a1719ee9a36799"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "wikitext-2-raw-v1/wikitext-2-raw-v1-trai(…):   0%|          | 0.00/6.18M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "54f8bbdf3f1340d58ce36198fb0f571e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "wikitext-2-raw-v1/wikitext-2-raw-v1-vali(…):   0%|          | 0.00/641k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dd7411cdd0fe46e9937b4bad0a38ee00"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "wikitext-2-raw-v1/wikitext-2-raw-v1-test(…):   0%|          | 0.00/715k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "364dd88fa9304a28900876206f3dd279"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split:   0%|          | 0/629 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "49a97eeb72a64afe9e7dbbb9190ee0f4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating validation split:   0%|          | 0/60 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6a2f0255c0c74a7b9ad90853d1f5aa2f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating test split:   0%|          | 0/62 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "df3c8500ad7f4468815648693de092d2"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"EleutherAI/wikitext_document_level\", \"wikitext-2-raw-v1\", split=\"train\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fo0mMfDoxKb-"
      },
      "source": [
        "## 1.2 Tokenize & Chunk the Dataset    \n",
        "\n",
        "For pretraining language models, a simple approach to tokenizing and chunking text is as follows:    \n",
        "1. Concatenate all the text into one giant \"blob\". This means you have one large string.\n",
        "2. Tokenize the whole blob into one list of tokens. At this point you have one large array of integers.\n",
        "3. Chunk the tokens into fixed size blocks (1024, 2048, larger...) (this is the \"context window\"). At this point you have multiple arrays of integers, each of the same length (context size).\n",
        "\n",
        "\n",
        "*This process will change slightly when using datasets that are too large to fit into memory.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZSWLtuWlxKb-"
      },
      "source": [
        "### 1.2.1 Tokenizing: Using Tiktoken\n",
        "\n",
        "One easy way to tokenize our dataset is to use OpenAI's tokenizer implementation tiktoken for BPE (Byte Pair Encoding) [8]. This article will not go into detail on how the implementation of a tokenizer works, but just know that it converts strings of text into lists of integers, and can also convert the lists of integers back into strings of texts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "LEDAnZUzxKb-",
        "outputId": "927bf215-aea2-4623-ce9e-df6e68b067f8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size: 50257\n",
            "End of text token: 50256\n",
            "Example tokenization: [15496, 995, 0]\n",
            "Input Shape torch.Size([2, 4])\n",
            "Output Shape torch.Size([2, 4])\n",
            "Input Example:\n",
            "tensor([[   27,  7700,    29,   220],\n",
            "        [  569, 18354,  7496, 17740]], device='cuda:0')\n",
            "Output Example:\n",
            "tensor([[ 7700,    29,   220,   796],\n",
            "        [18354,  7496, 17740,  6711]], device='cuda:0')\n"
          ]
        }
      ],
      "source": [
        "import tiktoken\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\") # Get the same tokenizer used for GPT-2\n",
        "\n",
        "\n",
        "print(\"Vocabulary size:\", tokenizer.n_vocab) # Vocabilary size is how many unique tokens the tokenizer can encode\n",
        "print(\"End of text token:\", tokenizer.eot_token) # End of text token is used to indicate the end of a text sequence\n",
        "print(\"Example tokenization:\", tokenizer.encode(\"Hello world!\"))\n",
        "\n",
        "# Convert entire dataset into a single string\n",
        "# This dataset is small enough to fit into memory\n",
        "# For larger datasets, you may need to use more\n",
        "# sophisticated methods to process the data.\n",
        "all_text = \"\"\n",
        "all_data = dataset[\"page\"]\n",
        "for example in all_data:\n",
        "    all_text += \"<page> \"+ example + \" </page>\"\n",
        "\n",
        "# Tokenize the entire text at once\n",
        "tokenized_text = tokenizer.encode(all_text)\n",
        "\n",
        "\n",
        "# We will create a function that generates a dataset of examples\n",
        "# for the language model. The function will take in the number of\n",
        "# examples to generate, the block size, and the test split.\n",
        "# It will return the training and test datasets.\n",
        "def get_dataset(num_examples, context_window_length, test_split=0.1):\n",
        "    input_blocks = [] # List to store input sequences\n",
        "    target_blocks = [] # List to store target sequences\n",
        "\n",
        "    # Use a sliding window to create input/target sequences\n",
        "    for i in range(0, len(tokenized_text), context_window_length + 1):\n",
        "        block = tokenized_text[i:i+context_window_length+ 1]\n",
        "\n",
        "        # Skip blocks that are too short\n",
        "        if len(block) < context_window_length + 1:\n",
        "            continue\n",
        "\n",
        "        input_seq = block[:-1]\n",
        "        target_seq = block[1:]\n",
        "\n",
        "        input_blocks.append(input_seq)\n",
        "        target_blocks.append(target_seq)\n",
        "\n",
        "        # Stop if we have enough examples\n",
        "        if len(input_blocks) >= num_examples:\n",
        "            break\n",
        "\n",
        "    # Convert to tensors for pytorch and move to gpu\n",
        "    inputs = torch.tensor(input_blocks, dtype=torch.long).to(device)\n",
        "    targets = torch.tensor(target_blocks, dtype=torch.long).to(device)\n",
        "\n",
        "    # Calculate train/test split point\n",
        "    split_idx = int(num_examples * (1 - test_split))\n",
        "\n",
        "    # Split into train/test\n",
        "    train_inputs = inputs[:split_idx]\n",
        "    train_targets = targets[:split_idx]\n",
        "    test_inputs = inputs[split_idx:]\n",
        "    test_targets = targets[split_idx:]\n",
        "    return train_inputs, train_targets, test_inputs, test_targets\n",
        "\n",
        "# Get a small dataset\n",
        "i, o, _, _ = get_dataset(2, 4, 0)\n",
        "print(\"Input Shape\", i.shape)\n",
        "print(\"Output Shape\", o.shape)\n",
        "print(\"Input Example:\")\n",
        "print(i)\n",
        "print(\"Output Example:\")\n",
        "print(o)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VMH2T4JNxKb-"
      },
      "source": [
        "Using our tokenizer methods, we have generated a \"dummy\" dataset that will be used for the rest of the diagrams / examples of the article to show the shapes of the matrices as they flow through the model.    \n",
        "\n",
        "- the input shape is $2x4$ - batch size x tokens    \n",
        "- the output shape is $2x4$ - batch size x tokens    \n",
        "\n",
        "This means that we have a context length of 4 tokens, and a batch size of 2. The full dummy dataset has a total of 2 examples. This is far smaller than the dataset would be in reality - but is useful for introducing the architecture.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8bgPJe6KxKb-"
      },
      "source": [
        "## 1.3 Build the LLM\n",
        "\n",
        "Now that we have a small dummy dataset. We can build our LLM model architecture in pytorch.\n",
        "\n",
        "### 1.3.1 Config Object\n",
        "First, we can build a \"config\" object that will store our parameters for the network. We will go through each parameter in depth later on in the network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "6B__omgbxKb_"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# A simple configuration container\n",
        "class GPTConfig:\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab_size,  # size of the vocabulary, from tokenizer, for gpt2 tokenizer it is 50257\n",
        "        n_layer,   # number of transformer blocks\n",
        "        n_head,    # number of attention heads for each transformer block\n",
        "        n_embd,  # embedding dimension for each token\n",
        "        seq_len,  # sequence length for the model - e.g. the \"context window\"\n",
        "\n",
        "    ):\n",
        "        self.vocab_size = vocab_size\n",
        "        self.n_layer = n_layer\n",
        "        self.n_head = n_head\n",
        "        self.n_embd = n_embd\n",
        "        self.seq_len = seq_len\n",
        "\n",
        "test_config = GPTConfig(\n",
        "    vocab_size=tokenizer.n_vocab,\n",
        "    n_layer=2,\n",
        "    n_head=3,\n",
        "    n_embd=6,\n",
        "    seq_len=4,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E7NCPBuvxKb_"
      },
      "source": [
        "### 1.3.2 Token Embedding Layer\n",
        "\n",
        "<div style=\"width:400px;margin:auto;\">\n",
        "\n",
        "![image-5.png](attachment:image-5.png)\n",
        "\n",
        "</div>\n",
        "\n",
        "Our first layer of the network is going to be a **token embedding layer**. This layer is a little bit different than traditional neural network layers. It is essentially a lookup table that returns an \"embedding vector\" for a given integer index. **The goal of this layer is to convert tokens to vectors**. These vectors are tuned as the network is trained so that their position in space relative to the other tokens reflects their statistical relationships with each other.    \n",
        "\n",
        "The embedding layer converts a discrete token (integer) into a semantic representation of that token (vector). Before the embedding layer, the model has no idea of what the token means or how it relates to other tokens. After the embedding layer, the model understands the semantic meaning of the token by its relationship with other tokens in the embedding space. For more information on word embeddings see the [Word2Vec](https://arxiv.org/pdf/1301.3781) paper [13].\n",
        "\n",
        "These are vectors that start off as random, but slowly assume values within embedding space that reflect the semantic meaning of the token. This process happens during training.\n",
        "\n",
        "<div style=\"width:800px;margin:auto;\">\n",
        "\n",
        "![image-3.png](attachment:image-3.png)\n",
        "\n",
        "</div>\n",
        "\n",
        "\n",
        "For our dummy dataset, the input to this layer will be a matrix of size $2x4$, batch x token indices. The output will be $2x4x6$, batch x tokens x embedding dimensions. This transformation can be visuzlized as follows:\n",
        "\n",
        "<div style=\"width:600px;margin:auto;\">\n",
        "\n",
        "![image-4.png](attachment:image-4.png)\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "sRgeniTCxKb_",
        "outputId": "08f9964b-4252-41ee-ddab-e010a1a6ba22",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch shape: torch.Size([2, 4]) Batch x Seq Len\n",
            "After embedding: torch.Size([2, 4, 6]) Batch x Seq Len x Embedding Dim\n",
            "\n",
            "Before embedding\n",
            "tensor([[   27,  7700,    29,   220],\n",
            "        [  569, 18354,  7496, 17740]], device='cuda:0')\n",
            "After embedding\n",
            "tensor([[[ 2.5756,  0.7956,  1.3269, -0.0701,  0.2557, -1.7040],\n",
            "         [ 0.3761,  0.3449, -0.4625, -0.1144,  0.3405,  0.6908],\n",
            "         [ 0.0259,  0.1465, -1.2370, -1.2495,  0.6761,  0.2933],\n",
            "         [-1.4681,  0.0101, -0.3941,  0.2006, -1.4398,  0.7329]],\n",
            "\n",
            "        [[ 0.1540, -0.1243,  0.1083, -0.2553,  0.2630,  2.5481],\n",
            "         [ 0.6572,  0.0729,  0.0478,  0.8017, -0.7443,  0.2907],\n",
            "         [-0.4462,  1.1941, -0.8174, -0.6028, -0.1100,  0.2223],\n",
            "         [-0.4762, -0.4669,  0.9769, -0.1284, -1.0017, -0.2041]]],\n",
            "       device='cuda:0', grad_fn=<EmbeddingBackward0>)\n"
          ]
        }
      ],
      "source": [
        "token_embedding = nn.Embedding(test_config.vocab_size, test_config.n_embd).to(device)\n",
        "test_batch_inputs, _, _, _ = get_dataset(2, test_config.seq_len, 0)\n",
        "print(\"Batch shape:\", test_batch_inputs.shape, \"Batch x Seq Len\")\n",
        "print(\"After embedding:\", token_embedding(test_batch_inputs).shape, \"Batch x Seq Len x Embedding Dim\")\n",
        "print(\"\")\n",
        "print(\"Before embedding\")\n",
        "print(test_batch_inputs)\n",
        "print(\"After embedding\")\n",
        "print(token_embedding(test_batch_inputs))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kBFJP2LsxKb_"
      },
      "source": [
        "In this example, we are using an embedding dimension of 6, so each original token is mapped to a vector of length 6. As of right now, these vectors don't have any actual meaning, they are randomly initialized. However, during the training process, these entries will be slowly nudged via backpropagation and over time they will start to assume meaning for their respective tokens."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_usZh6txKb_"
      },
      "source": [
        "### 1.3.3 Positional Encoding Layer\n",
        "\n",
        "<div style=\"width:400px;margin:auto;\">\n",
        "\n",
        "![image-2.png](attachment:image-2.png)\n",
        "\n",
        "</div>\n",
        "\n",
        "After embedding the tokens into embedding vectors, we will add a positional encoding to the vectors. Why do we need a positional encoding? Consider the following sentence:   \n",
        "\n",
        "*The planet is smaller than the other planet.*\n",
        "\n",
        "A positional encoding allows the model to differentiate the two instances of the word \"planet\". Without a positional encoding, the two token embedding vectors for each instance of the word planet would be exactly the same. Having a positional encoding allows the model to differentiate the two usages within the same instance.   \n",
        "\n",
        "We will use the positional encoding formula that was used in the original transformer paper [9]. The formula works by starting out with a matrix of shape sequence length x embedding dimension. The matrix is then filled in with the following formula:\n",
        "\n",
        "$$PE(POS,2i) = sin(\\frac{pos}{10000^\\frac{2i}{d}})$$\n",
        "$$PE(POS,2i+1) = cos(\\frac{pos}{10000^\\frac{2i}{d}})$$\n",
        "\n",
        "Where $POS$ is the position of the token in the sequence, i is the index of the embedding dimension within the token, and d is the embedding dimension size of the model. This entire formula outputs a matrix, and the matrix that it outputs is dependent on the embedding size. The resulting matrix will be (seq_length x embedding size). The matrix starts out as all zeros, and then the formula is applied."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "xIjdsX22xKcA",
        "outputId": "45a0cac8-f989-4d1e-a3fb-23a65c3a8175",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Position encoding shape: torch.Size([1, 4, 6])\n"
          ]
        }
      ],
      "source": [
        "def get_position_encoding(seq_len, d, n=10000):\n",
        "    \"\"\"\n",
        "    Computes the positional encoding matrix of shape (seq_len, d).\n",
        "\n",
        "    Args:\n",
        "        seq_len (int): Length of the sequence.\n",
        "        d (int): Dimension of the embedding.\n",
        "        n (float): The base for the exponential term (default 10000 in many Transformer implementations).\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: A tensor of shape (seq_len, d) containing the positional encodings.\n",
        "    \"\"\"\n",
        "\n",
        "    P = torch.zeros(seq_len, d).to(device)\n",
        "    for pos in range(seq_len):\n",
        "        for i in range(0, d // 2):\n",
        "            P[pos, 2 * i] = math.sin(pos / (n ** ((2 * i) / d)))\n",
        "            if i + 1 < d:\n",
        "                P[pos, 2* i + 1] = math.cos(pos / (n ** ((2 * i) / d)))\n",
        "\n",
        "    return P.unsqueeze(0)\n",
        "\n",
        "\n",
        "# Example usage:\n",
        "position_encoding = get_position_encoding(seq_len=test_config.seq_len, d=test_config.n_embd)\n",
        "print(\"Position encoding shape:\", position_encoding.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dfcRW7ypxKcA"
      },
      "source": [
        "Once we have the positional encoding, we add that using element wise addition to the embedding vectors. Since we are using pytorch, the addition will \"broadcast\" across the first dimension. This means that the 4x6 positional encoding matrix will be added to each batch example in parallel.   \n",
        "\n",
        "<div style=\"width:800px;margin:auto;\">\n",
        "\n",
        "![image.png](attachment:image.png)\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Ps7T0GeExKcA",
        "outputId": "f2b16282-0ada-4fe3-ea87-f65af0cd55ab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token embeddings shape: torch.Size([2, 4, 6])\n",
            "Position encodings shape: torch.Size([1, 4, 6])\n",
            "Sum of token embeddings and position encodings: torch.Size([2, 4, 6])\n"
          ]
        }
      ],
      "source": [
        "test_embeddings = token_embedding(test_batch_inputs)\n",
        "test_embeddings_with_pos = test_embeddings + position_encoding\n",
        "print(\"Token embeddings shape:\", test_embeddings.shape)\n",
        "print(\"Position encodings shape:\", position_encoding.shape)\n",
        "print(\"Sum of token embeddings and position encodings:\",test_embeddings_with_pos.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hqt861sBxKcA"
      },
      "source": [
        "#### What is the Intuition Behind Positional Encodings?\n",
        "\n",
        "At first, it can be a challenging to intuit what the positional encoding is doing. The positional encoding is just a constant matrix (given the sequence length and embedding size), with the values set to a desirable pattern. Each row of the matrix aligns to a token, meaning a constant vector will be added to the token at position 1 every time, and a different constant vector added to the token at position 2 every time, etc...\n",
        "\n",
        "This differentiates the value of the word \"planet\" coming at the beginning vs the end of the sentence. However, sometimes relative position of words in a sentence is more important than absolute position. So how do we take that into account? The answer is that the relative relationships between words are emergent. These happen through the process of attention, which we will discuss later.\n",
        "\n",
        "The key point here is that without positional encoding, these two sentences would look the same:   \n",
        "- The dog chased the owner\n",
        "- The owner chased the dog    \n",
        "\n",
        "\n",
        "The positional encoding makes the vectors for dog and owner different in the two sentences, which allows attention to catch onto the relative relationships between these two words.\n",
        "\n",
        "The below image shows an example of a positional encoding matrix. It looks interesting but what exactly are we looking at? Why does this help the model encode the position of each embedding vector. Remember, each row in our embedding vector represents a word/token. We will be adding this matrix to the embedding matrix to encode positions. One thing to note about this matrix is that each row is unique. There is also a smooth transition between each row. If you take rows 27 and 28 from this matrix, they are going to have very similar patterns. However if you take rows 1 and 120 from this matrix, they are going to differ much more. This smoothness is also an important feature that helps the model understand position [10].\n",
        "\n",
        "<div style=\"width:500px;margin:auto;\">\n",
        "\n",
        "![image.png](attachment:image.png)\n",
        "\n",
        "</div>\n",
        "\n",
        "There is nothing inherently special about the formula above, there are other formulas for positional encoding. The key thing to note is that there needs to be some matrix that we can add to our embedding matrix that encodes position. This formula has certain properties that are biased towards making it easy for the model to do that.   \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sMetdzXkxKcA"
      },
      "source": [
        "### 1.3.4 Masked Multiheaded Self Attention\n",
        "\n",
        "<div style=\"width:400px;margin:auto;\">\n",
        "\n",
        "![image-9.png](attachment:image-9.png)\n",
        "\n",
        "</div>\n",
        "\n",
        "After positional encoding, we get to the core of the LLM - the (decoder only) transformer. The first step of the transformer is masked multiheaded self attention. We can break down the internals of the transformer into three parts: self attention, then masking, then the multiple heads.    \n",
        "\n",
        "#### Self Attention\n",
        "The core idea behind self attention is that it allows every token to \"talk\" to the other tokens. Attention \"reframes\" a word's meaning into a combination of all the other words in the context window. A single self attention head does one of many possible \"reframings\" of each token. It allows for the model to understand a each word's context in relation to the other words of the sentence.\n",
        "\n",
        "Self attention starts with just the token embedding matrix with position encodings. It \"decomposes\" this matrix into queries, keys, and values. In reality all of these are just vectors / matrices that get tuned during training, but we can conceptually think of them as queries, keys, and values due to their dot product operations that take place in the attention operation.\n",
        "\n",
        "The original equation for scaled dot product attention is as follows [9]:\n",
        "$$Attention(Q,K,V)=softmax(\\frac{QK^t}{\\sqrt{d_k}})V$$\n",
        "\n",
        "Q, K, and V are query, key, and value matrices. They are set initially through matrix projections of the input embedding matrix. The token embeddings are multplied by $W_q$, $W_k$, and $W_v$ matrices. These weight matrices start off as random and are tuned during the process of training the network. Meaning during training, the network learns what \"queries\" to ask, and what \"keys\" and \"values\" to set via backpropagation by tuning these matrices. It learns how to transform the embedding matrix into \"keys\", \"queries\", and \"values\" in order to best reduce the loss of the network.    \n",
        "\n",
        "The projection operation to generate Q,K, and V are shown below using the dimensions for our dummy dataset/network.\n",
        "\n",
        "<div style=\"width:700px;margin:auto;\">\n",
        "\n",
        "![image-3.png](attachment:image-3.png)\n",
        "\n",
        "</div>\n",
        "\n",
        "Q, K, and V are all matrices that are of shape *num tokens x embedding size*. Each token has a query vector in \"query space\". Each token also has a key vector in \"key space\". When we do the $QK^T$ operation, we are calculating how well each token query matches each key. This could be thought of as sort of a \"fuzzy lookup\" using vector dot products. If the query and key have a high dot product, that means the vectors are pointing in a direction near each other. This also means those two tokens are important to take into account together.    \n",
        "\n",
        "After doing the matrix multiplication between $Q$ and $K^T$, we end up with a similarity matrix of tokens. This similarity matrix tells us how much each token attends to each other token. Each row of the $QK^T$ matrix is put through the softmax function so each row becomes a probability distribution that adds to one. This probability distribution can be interpreted as how strong of a match each key is to the query of the row. How much each key \"attends\" to each query.\n",
        "\n",
        "The value matrix can be thought of the actual content/information that the each token has to offer. This value matrix is weighted by the similarities of the keys/queries to produce the final output of self attention.   \n",
        "\n",
        "\n",
        "#### Self Attention: Further Intuition\n",
        "\n",
        "There are some alternative ways to conceive of the individual operations of attention that can help at a conceptual / intuitive level to know what the network is doing. Let's go through each operation in attention and try to simplify down in english what it is doing at a conceptual level.    \n",
        "\n",
        "##### Q, K, V Matrices Intuition\n",
        "We know that the $Q$, $K$, $V$ matrices are created by a matrix operation to the input of the transformer (for the first block, this is our position encoded word embeddings). We also know that the weights to create these matrices are tuned through the process of backpropagation. But how can we think of these matrices themselves? What information do they actually contain?   \n",
        "\n",
        "##### $Q$ Matrix Intuition\n",
        "\n",
        "The $Q$ matrix can be thought of as n rows of queries or questions, where n is the number of tokens in the input. When thinking about the $Q$ matrix, think of it as n vectors instead of a single matrix. Where each vector is a query or question about the corresponding word that could be answered by some combinations of the other words. Remember, we are \"reframing\" the give word as some combination of the other words. For example it could look like the following:   \n",
        "\n",
        "<div style=\"width:700px;margin:auto;\">\n",
        "\n",
        "![image.png](attachment:image.png)    \n",
        "\n",
        "</div>\n",
        "\n",
        "In this case each token has a corresponding question. These questions or queries are going to be questions that can be answered by the surrounding tokens. So how are these questions created? $W_q$ is responsible for creating the right questions for each token (with position). $W_q$ maps a token to a relevant query about that token. These queries become relevant through the process of training via backpropagation.    \n",
        "\n",
        "\n",
        "##### $K$ Matrix Intuition\n",
        "We can think of the $K$ matrix as n row vectors of keys, where n is the number of tokens in the input. What do we mean by \"keys\". It is easiest to think of keys as facts that can help answer queries. Above in the query section we asked questions like \"what noun do I describe?\". A key that might closely match this query would be \"I am a noun that can be described\". Similar to the queries, $W_k$ creates these keys by learning the right mapping from token to corresponding key. These keys are good matches for the queries becuase of the $QK^T$ operation that is performed in training.\n",
        "\n",
        "<div style=\"width:700px;margin:auto;\">\n",
        "\n",
        "![image-5.png](attachment:image-5.png)\n",
        "\n",
        "</div>\n",
        "\n",
        "Overall, each key can be conceived of as a fact about that token that could help answer a queries that the other tokens might have.\n",
        "\n",
        "\n",
        "##### $QK^T$ Operation Intuition:\n",
        "\n",
        "Now that we have an intuition of the $Q$ and $K$ matrix, we can think about what the matrix multiplication operation $QK^T$ in the attention equation is doing. The $QK^T$ operation is a matching  operation, where each query is compared with each key, by performing a dot product operation. If the dot product is large, that means that the key answers or \"attends\" to the query. If the dot product is small, that means the key is unrelated and does not help answer the query. The $QK^T$ operation \"reframes\" each query into a set of keys. The resulting matrix of the operation can be thought of as n row vectors. Every dimension or coordinate of these row vectors is a weight for a token key/fact. So a vector in this space is some weighted combination of all of the tokens (keys).   \n",
        "\n",
        "Basically, what we are doing is redescribing the original token query/question as a weighted vector of all of the token keys/answers. Instead of asking a question about of token, we have n different answers, all with their own weights.    \n",
        "\n",
        "When doing the $QK^T$ operation, we are reframing the query row vectors to a combination of the keys. Remember each query has to do with how that token relates to the other tokens, so the answers can be formed as some combination of the other tokens.\n",
        "\n",
        "<div style=\"width:700px;margin:auto;\">\n",
        "\n",
        "![image-4.png](attachment:image-4.png)\n",
        "\n",
        "</div>\n",
        "\n",
        "##### $\\frac{QK^T}{\\sqrt{d_k}}$ Operation Intuition:\n",
        "\n",
        "This operation is done to make the output of the softmax more stable. The dot product of two random vectors of dimension $d_k$ results in values that tend to grow proportionally to $d_k$. This ensures that no matter, how large $d_k$ is, the softmax works as expected and does not result in extreme values.   \n",
        "\n",
        "This is an elementwise division so every element of the matrix is divided by this value. The resulting matrix can be thought of in the same way as the $QK^T$ result, just scaled.\n",
        "\n",
        "##### $softmax(\\frac{QK^T}{\\sqrt{d_k}})$ Operation Intuition:\n",
        "\n",
        "The softmax operation is performed row-wise on the $\\frac{QK^T}{\\sqrt{d_k}}$ matrix. This means every row results in a probability distribution.\n",
        "We can still think of this as each token is represented as a \"reframed\" query vector, but now we know that each row vector adds up to one.\n",
        "\n",
        "##### $V$ matrix Intuition\n",
        "\n",
        "The $V$ matrix is a bit hard to conceive of, but can be thought of as a column matrix, where each column is a learned feature, and each element of those vectors is the value of that feature for the token in that row. They are \"feature\" vectors, that contain information about specific learned features for each token. When we do the final operation, these feature vectors will be weighted, meaning that the values of these features for certain tokens on should be focused on more than other tokens. The $V$ matrix is the actual content or output of attention. This content will be  adjusted by the weights from the $softmax(\\frac{QK^T}{\\sqrt{d_k}})$ operation\n",
        "\n",
        "<div style=\"width:300px;margin:auto;\">\n",
        "\n",
        "![image-8.png](attachment:image-8.png)\n",
        "\n",
        "</div>\n",
        "\n",
        "\n",
        "##### $softmax(\\frac{QK^T}{\\sqrt{d_k}})V$ Operation Intuition:\n",
        "\n",
        "Now for the final operation of attention, multiplying by the $V$ matrix. We can think of the V matrix as containing the original content of the embeddings. We weight this content based on the query/key matches. In other words, we weight the content based on the specific questions we are trying to ask and how the other words in context answer those questions.\n",
        "\n",
        "\n",
        "$$softmax(\\frac{QK^T}{\\sqrt{d_k}})V$$\n",
        "\n",
        "<div style=\"width:900px;margin:auto;\">\n",
        "\n",
        "![image-7.png](attachment:image-7.png)\n",
        "\n",
        "</div>\n",
        "\n",
        "When putting this all together (using the original dimensions of our \"test\" config object as we are in the code), we can see what all the matrix operations and dimensions through the self attention operation are.     \n",
        "   \n",
        "\n",
        "<div style=\"width:1000px;margin:auto;\">\n",
        "\n",
        "![image-6.png](attachment:image-6.png)\n",
        "\n",
        "</div>\n",
        "\n",
        "#### Self Attention: Code\n",
        "\n",
        "Self attention can be written as a self contained pytorch module as shown below.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "g3eP7ldtxKcA",
        "outputId": "460eaff0-2ecb-4c14-cc8b-3ef14e6e4e9b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention input shape: torch.Size([2, 4, 6])\n",
            "\n",
            "Query weights shape: torch.Size([6, 6])\n",
            "Key weights shape: torch.Size([6, 6])\n",
            "Value weights shape: torch.Size([6, 6])\n",
            "\n",
            "Queries shape: torch.Size([2, 4, 6])\n",
            "Keys shape: torch.Size([2, 4, 6])\n",
            "Values shape: torch.Size([2, 4, 6])\n",
            "\n",
            "QK^T shape: torch.Size([2, 4, 4])\n",
            "\n",
            "Attention output shape: torch.Size([2, 4, 6])\n"
          ]
        }
      ],
      "source": [
        "class SelfAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.Wq = nn.Parameter(torch.randn(config.n_embd, config.n_embd)).to(device) # Query weights - will transform input embeddings into queries\n",
        "        self.Wk = nn.Parameter(torch.randn(config.n_embd, config.n_embd)).to(device) # Key weights - will transform input embeddings into keys\n",
        "        self.Wv = nn.Parameter(torch.randn(config.n_embd, config.n_embd)).to(device) # Value weights - will transform input embeddings into values\n",
        "\n",
        "    def forward(self, x):\n",
        "        print(\"Attention input shape:\", x.shape)\n",
        "        print(\"\")\n",
        "        print(\"Query weights shape:\", self.Wq.shape)\n",
        "        print(\"Key weights shape:\", self.Wk.shape)\n",
        "        print(\"Value weights shape:\", self.Wv.shape)\n",
        "        queries = x @ self.Wq # Matrix multiplication to transform input embeddings into queries\n",
        "        keys = x @ self.Wk # Matrix multiplication to transform input embeddings into keys\n",
        "        values = x @ self.Wv # Matrix multiplication to transform input embeddings into values\n",
        "        print(\"\")\n",
        "        print(\"Queries shape:\", queries.shape)\n",
        "        print(\"Keys shape:\", keys.shape)\n",
        "        print(\"Values shape:\", values.shape)\n",
        "\n",
        "        qkt = queries @ keys.transpose(-2, -1) # Calculate QK^T\n",
        "        qkt_scaled = qkt / math.sqrt(queries.size(-1)) # Scale QK^T by the dimension of the keys\n",
        "        qkt_softmax = F.softmax(qkt_scaled, dim=-1) # Apply softmax row-wise to get attention weights\n",
        "        print(\"\")\n",
        "        print(\"QK^T shape:\", qkt.shape)\n",
        "\n",
        "        attn_output = qkt_softmax @ values # Multiply softmax(QK^T) by values\n",
        "        print(\"\")\n",
        "        print(\"Attention output shape:\", attn_output.shape)\n",
        "        return attn_output\n",
        "\n",
        "attention = SelfAttention(test_config)\n",
        "test_out = attention(test_embeddings_with_pos)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Xirwn4sxKcA"
      },
      "source": [
        "#### Causal Self Attention\n",
        "\n",
        "Now that we have implemented self attention, we can move on to causal self attention. During training, we are trying to predict the next token at each time step in parallel in the transformer. However, we will be cheating if we allow attention to see future tokens during the training process. It will just predict the future tokens by looking at them. For this reason we need to mask the matrices so that future tokens are hidden from self attention layers. We perform this masking after the $QK^T$ operation [11].\n",
        "\n",
        "<div style=\"width:400px;margin:auto;\">\n",
        "\n",
        "![image.png](attachment:image.png)\n",
        "\n",
        "</div>\n",
        "\n",
        "The masking process makes the output of the softmax operation 0 in the upper right corner. This makes it to where the following occurs:\n",
        "- The query for token 1 is only able to be reframed as a combination of token 1\n",
        "- The query for token 2 is only able to be reframed as a combination of tokens 1 and 2\n",
        "- The query for token 3 is only able to be reframed as a combination of tokens 1,2, and 3\n",
        "- The query for token 4 is only able to be reframed as a combination of tokens 1,2,3, and 4\n",
        "- etc...\n",
        "\n",
        "*When we say the query is able to be reframed, what we mean mathematically is that the value in that matrix entry could possibly be over 0.*\n",
        "\n",
        "We can modify our self attention block above to add masking with the following changes:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Hi-fGnvGxKcA",
        "outputId": "262126ee-d829-446f-c0b4-03e141d461b7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 4, 6])\n"
          ]
        }
      ],
      "source": [
        "class CausalSelfAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.Wq = nn.Parameter(torch.randn(config.n_embd, config.n_embd)).to(device) # Query weights - will transform input embeddings into queries\n",
        "        self.Wk = nn.Parameter(torch.randn(config.n_embd, config.n_embd)).to(device) # Key weights - will transform input embeddings into keys\n",
        "        self.Wv = nn.Parameter(torch.randn(config.n_embd, config.n_embd)).to(device) # Value weights - will transform input embeddings into values\n",
        "\n",
        "    def forward(self, x):\n",
        "        seq_len = x.shape[1] # Get sequence length (number of tokens / context window length)\n",
        "        queries = x @ self.Wq # Matrix multiplication to transform input embeddings into queries\n",
        "        keys = x @ self.Wk    # Matrix multiplication to transform input embeddings into keys\n",
        "        values = x @ self.Wv  # Matrix multiplication to transform input embeddings into values\n",
        "        qkt = queries @ keys.transpose(-2, -1)  # Calculate QK^T\n",
        "        qkt_scaled = qkt / math.sqrt(queries.size(-1))  # Scale QK^T by the dimension of the keys\n",
        "\n",
        "        # MASKING\n",
        "        # THIS IS THE ONLY DIFFERENCE, USE -inf FOR UPPER TRIANGLE MASK SO THAT SOFTMAX WILL BE 0\n",
        "        causal_mask = torch.triu(torch.ones(seq_len, seq_len, device=x.device), diagonal=1)\n",
        "        causal_mask = causal_mask.masked_fill(causal_mask == 1, float('-inf'))  # Upper triangle masked with -inf\n",
        "        qkt_scaled = qkt_scaled + causal_mask # Add the mask to the scaled QK^T\n",
        "        # END MASKING\n",
        "\n",
        "        qkt_softmax = F.softmax(qkt_scaled, dim=-1) # Apply softmax row-wise to get attention weights, the -inf values will become 0 here\n",
        "        attn_output = qkt_softmax @ values # Multiply softmax(QK^T) by values\n",
        "        return attn_output\n",
        "\n",
        "\n",
        "attention = CausalSelfAttention(test_config)\n",
        "test_out = attention(test_embeddings_with_pos)\n",
        "print(test_out.shape)  # Output should have shape: (batch_size, seq_len, n_embd)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DwaTFOePxKcB"
      },
      "source": [
        "#### Multi-Headed Causal Self Attention\n",
        "\n",
        "Now we have causal self attention, we can add in the \"multi-headed\" part of the attention layer. We can do this by concatenating multiple CausalAttention operations together in parallel. We then add a layer to project the final output back down the the input size.  \n",
        "\n",
        "##### Multi-Headed Causal Self Attention intuition\n",
        "\n",
        "What is this actually doing conceptually? It is allowing each head to have the tokens attend to each other in different ways. For instance one head might be focusing on grammatical structure, another might be focusing on semantic meaning, while another based on real-world meaning. If viewing the sentence \"the sky is blue\" from a grammatical structure perspective, the word \"the\" might attend to the word \"sky\" heavily becuase that is what it is referring to. However if viewing attention through the lense of real-world meaning, the word \"the\" won't attend to the word \"sky\" very much becuase their meanings are not similar. Each word's relationship to the other words might be different depending on what \"lens\" (or \"head\") you are viewing them through.    \n",
        "\n",
        "To reiterate, this is a helpful conceptual way to think about multi-headed attention, but the meanings of each head is not always human understandable in this way. They are going take on whatever meaning helps minimize the loss function of the training set the most.   \n",
        "\n",
        "The final output of Multi-Headed Causal Self Attention is the exact same size as the input, becuase of the final feedforward layer that projects the concatenated attention outputs back down.\n",
        "\n",
        "\n",
        "##### Multi-Headed Causal Self Attention Code\n",
        "\n",
        "The following code snippet shows an implementation of multi-headed causal self attention, building on our previous attention blocks. This is not the most compute efficient implementation due to the for loop for each head, but it is easier to read than the fully vectorized version and works for our use case due to the small datasets we are using."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "CfsVE8eLxKcB",
        "outputId": "585e9b83-86de-4136-9f04-0ae469f8f105",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 4, 6])\n"
          ]
        }
      ],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.attn_heads = nn.ModuleList([\n",
        "            CausalSelfAttention(config) for _ in range(config.n_head)\n",
        "        ])  # Create n_head attention heads\n",
        "        self.projection = nn.Linear(config.n_embd * config.n_head, config.n_embd).to(device) # Linear layer to project multi-head attention outputs\n",
        "\n",
        "    def forward(self, x):\n",
        "        head_outputs = [head(x) for head in self.attn_heads] # Get the output of each attention head\n",
        "        multihead_output = torch.cat(head_outputs, dim=-1) # Concatenate the outputs\n",
        "        return self.projection(multihead_output) # Project the concatenated outputs\n",
        "\n",
        "multihead_attn = MultiHeadAttention(test_config)\n",
        "test_out = multihead_attn(test_embeddings_with_pos)\n",
        "print(test_out.shape)  # Output should have shape: (batch_size, seq_len, n_embd)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K4Kp6eTrxKcB"
      },
      "source": [
        "### 1.3.5 The Block\n",
        "\n",
        "We have now succesfully implemented multi-headed attention. There are just a few steps left until we have a GPT \"block\" that we can stack onto the network over and over again. The architecture of a GPT block is as follows:\n",
        "\n",
        "<div style=\"width:400px;margin:auto\">\n",
        "\n",
        "![image-2.png](attachment:image-2.png)\n",
        "\n",
        "</div>\n",
        "\n",
        "So far we have built the text embedding, positional encoding, and masked multiheaded self attention parts. Now we need to add in the normalization layers and the feedforward layers. These are straightforward pytorch layers that are common across many neural network architectures.\n",
        "\n",
        "### Layer normalization layers\n",
        "The layer normalization layers are straghtforward and used in many deep learning architectures. It normalizes the values of the incoming matrix across the feature dimension (in our case dimension 2). It is used to stabilize training and achieve faster convergence.\n",
        "\n",
        "#### Feedforward layer\n",
        "The feedforward layer of the transformer block operates with a different paradigm than attention. While attention captures relationships between tokens, the feedforward layer applies the same transformation to each token in parallel. It can be implemented using standard pytorch linear layers. We are using a factor of 4 x embedding dimension for the size of the linear layer, as was done in the original attention is all you need paper. We use the Gaussian Error Linear Unit (GELU) activation function as is implemented in the original GPT paper."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "pAqMygqMxKcB",
        "outputId": "1d5ae799-fdb1-4946-97e1-512c75c2e47a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 4, 6])\n"
          ]
        }
      ],
      "source": [
        "class GPTBlock(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.mha = MultiHeadAttention(config)\n",
        "        self.ln1 = nn.LayerNorm(config.n_embd).to(device)\n",
        "        self.ffn = nn.Sequential(\n",
        "            nn.Linear(config.n_embd, 4 * config.n_embd),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(4 * config.n_embd, config.n_embd),\n",
        "        ).to(device)\n",
        "        self.ln2 = nn.LayerNorm(config.n_embd).to(device)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.mha(self.ln1(x))\n",
        "        x = x + self.ffn(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "block = GPTBlock(test_config)\n",
        "test_out = block(test_embeddings_with_pos)\n",
        "print(test_out.shape)  # Output should have shape: (batch_size, seq_len, n_embd)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQkhSdhGxKcB"
      },
      "source": [
        "### 1.3.6 Putting it All Together\n",
        "\n",
        "\n",
        "Now that we have a block, we can stack the blocks together multiple times to have a GPT style LLM model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "8WDuOmOXxKcB",
        "outputId": "4f6c77a4-c61f-4ae8-f52c-1e764e2149c1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 4])\n",
            "torch.Size([2, 4, 50257])\n"
          ]
        }
      ],
      "source": [
        "class GPTModel(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.token_embedding = nn.Embedding(config.vocab_size, config.n_embd).to(device)\n",
        "        self.position_encoding = get_position_encoding(config.seq_len, config.n_embd)\n",
        "        self.blocks = nn.Sequential(*[GPTBlock(config) for _ in range(config.n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(config.n_embd).to(device)\n",
        "        self.head = nn.Linear(config.n_embd, config.vocab_size).to(device)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.token_embedding(x) + self.position_encoding\n",
        "        x = self.blocks(x)\n",
        "        x = self.ln_f(x)\n",
        "        return self.head(x)\n",
        "\n",
        "gpt = GPTModel(test_config)\n",
        "print(test_batch_inputs.shape)\n",
        "test_out = gpt(test_batch_inputs)\n",
        "print(test_out.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ulj-A4F1xKcB"
      },
      "source": [
        "That is a full forward pass through the LLM, the input is of shape $[batch,tokens]$ and the output is of shape $[batch,tokens,probabilities]$. For each token given in the input, the LLM will predict a discrete probability distribution of the next token that comes after that.\n",
        "\n",
        "The transformer makes multiple predictions of this in parallel, one for each token in the input. While all of them are used in training, only the last prediction (of token n) is used in inference to to the final predition.   \n",
        "\n",
        "The following diagram shows the full forward pass with shapes as one example moves through the matrix.\n",
        "\n",
        "<div style=\"width:600px;margin:auto\">\n",
        "\n",
        "![image-2.png](attachment:image-2.png)\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NAPkGcZmxKcB"
      },
      "source": [
        "### 1.3.7 Dummy Training Loop\n",
        "\n",
        "Now that we have gone through the forward pass of the model, we can train it. The model is trained using next token prediction\n",
        "\n",
        "#### Objective Function\n",
        "\n",
        "According to the original GPT paper, the objective function of pretraining is the following [1]:  \n",
        "\n",
        "$$L1(U) = \\sum_{i}logP(u_i|u_{i-k}...u_{i-1};\\theta)$$\n",
        "\n",
        "- $U$ is the sequence of text (tokens) we are computing the objective function for\n",
        "- $u_i$ is the current token of the sequence\n",
        "- $u_{i-k}...u_{i-1}$ are the previous k tokens (context window)\n",
        "- $P(u_i|u_{i-k}...u_{i-1};\\theta)$ is the probability of predicting $u_i$ given the previous tokens $u_{i-k}...u_{i-1}$ and the models's parameters $\\theta$\n",
        "- We take the log of this value for its useful properties in optimization\n",
        "- We then take the sum across all the tokens in the sequence\n",
        "- Therefore, in english, we can find the probability of predicting a token based on the previous k tokens and the models weights. We want to maximize this probability across all the tokens in the sequence.\n",
        "\n",
        "\n",
        "Maximizing this objective function is essentially the same as minimizing the cross entropy loss function.\n",
        "\n",
        "$$H(p, q) = -\\sum_{x} p(x) \\log q(x)$$\n",
        "\n",
        "- Where p(x) is the true class discrete probability distribution\n",
        "- q(x) is the predicted class discrete probability distribution\n",
        "\n",
        "This is becuase during training, we use a one hot encoded vector for the true distribution, so p(x) is 1 for the correct token, and 0 for all other tokens. This means we can remove the sum and simplify the cross entropy loss to this:\n",
        "\n",
        "\n",
        "$$H(p, q) = -\\log P(u_i \\mid u_{i-k}, \\dots, u_{i-1}; \\theta)$$\n",
        "\n",
        "Pytorch has a pre-built cross-entropy loss function that can be used as our criterion to minimize [12].\n",
        "\n",
        "\n",
        "#### Test One: Overfitting\n",
        "We will first train the model with a small dataset (10 examples) and see if we can get the model to memorize/overfit to the dataset. This is a good test to ensure that our architecture is correct and getting the loss to reduce as expected.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MpSouelFxKcB",
        "outputId": "6ea0cba9-a323-47b9-963d-bfc7e0ffc02e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 2/1000, Loss: 10.955514907836914, LR: 0.0005\n",
            "Step 3/1000, Loss: 10.85673713684082, LR: 0.0005\n",
            "Step 4/1000, Loss: 10.764195442199707, LR: 0.0005\n",
            "Step 5/1000, Loss: 10.669764518737793, LR: 0.0005\n",
            "Step 6/1000, Loss: 10.568231582641602, LR: 0.0005\n",
            "Step 7/1000, Loss: 10.457962036132812, LR: 0.0005\n",
            "Step 8/1000, Loss: 10.364540100097656, LR: 0.0005\n",
            "Step 9/1000, Loss: 10.2617826461792, LR: 0.0005\n",
            "Step 10/1000, Loss: 10.151437759399414, LR: 0.0005\n",
            "Step 11/1000, Loss: 10.062667846679688, LR: 0.0005\n",
            "Step 12/1000, Loss: 9.948381423950195, LR: 0.0005\n",
            "Step 13/1000, Loss: 9.839149475097656, LR: 0.0005\n",
            "Step 14/1000, Loss: 9.7377290725708, LR: 0.0005\n",
            "Step 15/1000, Loss: 9.628875732421875, LR: 0.0005\n",
            "Step 16/1000, Loss: 9.515869140625, LR: 0.0005\n",
            "Step 17/1000, Loss: 9.401544570922852, LR: 0.0005\n",
            "Step 18/1000, Loss: 9.285650253295898, LR: 0.0005\n",
            "Step 19/1000, Loss: 9.166109085083008, LR: 0.0005\n",
            "Step 20/1000, Loss: 9.043201446533203, LR: 0.0005\n",
            "Step 21/1000, Loss: 8.916478157043457, LR: 0.0005\n",
            "Step 22/1000, Loss: 8.794842720031738, LR: 0.0005\n",
            "Step 23/1000, Loss: 8.668807029724121, LR: 0.0005\n",
            "Step 24/1000, Loss: 8.553834915161133, LR: 0.0005\n",
            "Step 25/1000, Loss: 8.429769515991211, LR: 0.0005\n",
            "Step 26/1000, Loss: 8.311910629272461, LR: 0.0005\n",
            "Step 27/1000, Loss: 8.192338943481445, LR: 0.0005\n",
            "Step 28/1000, Loss: 8.07109260559082, LR: 0.0005\n",
            "Step 29/1000, Loss: 7.9453630447387695, LR: 0.0005\n",
            "Step 30/1000, Loss: 7.816563606262207, LR: 0.0005\n",
            "Step 31/1000, Loss: 7.693399906158447, LR: 0.0005\n",
            "Step 32/1000, Loss: 7.5673508644104, LR: 0.0005\n",
            "Step 33/1000, Loss: 7.444938659667969, LR: 0.0005\n",
            "Step 34/1000, Loss: 7.327301025390625, LR: 0.0005\n",
            "Step 35/1000, Loss: 7.209898948669434, LR: 0.0005\n",
            "Step 36/1000, Loss: 7.0977888107299805, LR: 0.0005\n",
            "Step 37/1000, Loss: 6.980617523193359, LR: 0.0005\n",
            "Step 38/1000, Loss: 6.868869781494141, LR: 0.0005\n",
            "Step 39/1000, Loss: 6.759540557861328, LR: 0.0005\n",
            "Step 40/1000, Loss: 6.659881591796875, LR: 0.0005\n",
            "Step 41/1000, Loss: 6.568659782409668, LR: 0.0005\n",
            "Step 42/1000, Loss: 6.484365940093994, LR: 0.0005\n",
            "Step 43/1000, Loss: 6.397005558013916, LR: 0.0005\n",
            "Step 44/1000, Loss: 6.311579704284668, LR: 0.0005\n",
            "Step 45/1000, Loss: 6.230798244476318, LR: 0.0005\n",
            "Step 46/1000, Loss: 6.156350612640381, LR: 0.0005\n",
            "Step 47/1000, Loss: 6.086760997772217, LR: 0.0005\n",
            "Step 48/1000, Loss: 6.021421909332275, LR: 0.0005\n",
            "Step 49/1000, Loss: 5.9562506675720215, LR: 0.0005\n",
            "Step 50/1000, Loss: 5.895166873931885, LR: 0.0005\n",
            "Step 51/1000, Loss: 5.837292671203613, LR: 0.0005\n",
            "Step 52/1000, Loss: 5.780357360839844, LR: 0.0005\n",
            "Step 53/1000, Loss: 5.727105140686035, LR: 0.0005\n",
            "Step 54/1000, Loss: 5.680415153503418, LR: 0.0005\n",
            "Step 55/1000, Loss: 5.631875038146973, LR: 0.0005\n",
            "Step 56/1000, Loss: 5.583223819732666, LR: 0.0005\n",
            "Step 57/1000, Loss: 5.533563613891602, LR: 0.0005\n",
            "Step 58/1000, Loss: 5.495665550231934, LR: 0.0005\n",
            "Step 59/1000, Loss: 5.46055269241333, LR: 0.0005\n",
            "Step 60/1000, Loss: 5.423654079437256, LR: 0.0005\n",
            "Step 61/1000, Loss: 5.386800765991211, LR: 0.0005\n",
            "Step 62/1000, Loss: 5.354977607727051, LR: 0.0005\n",
            "Step 63/1000, Loss: 5.315482139587402, LR: 0.0005\n",
            "Step 64/1000, Loss: 5.289336681365967, LR: 0.0005\n",
            "Step 65/1000, Loss: 5.263497352600098, LR: 0.0005\n",
            "Step 66/1000, Loss: 5.227795600891113, LR: 0.0005\n",
            "Step 67/1000, Loss: 5.201839447021484, LR: 0.0005\n",
            "Step 68/1000, Loss: 5.18137264251709, LR: 0.0005\n",
            "Step 69/1000, Loss: 5.162177085876465, LR: 0.0005\n",
            "Step 70/1000, Loss: 5.1388139724731445, LR: 0.0005\n",
            "Step 71/1000, Loss: 5.125182151794434, LR: 0.0005\n",
            "Step 72/1000, Loss: 5.102317810058594, LR: 0.0005\n",
            "Step 73/1000, Loss: 5.0763959884643555, LR: 0.0005\n",
            "Step 74/1000, Loss: 5.065499782562256, LR: 0.0005\n",
            "Step 75/1000, Loss: 5.052401065826416, LR: 0.0005\n",
            "Step 76/1000, Loss: 5.030683994293213, LR: 0.0005\n",
            "Step 77/1000, Loss: 5.019297122955322, LR: 0.0005\n",
            "Step 78/1000, Loss: 5.018438816070557, LR: 0.0005\n",
            "Step 79/1000, Loss: 5.01409387588501, LR: 0.0005\n",
            "Step 80/1000, Loss: 5.000054836273193, LR: 0.0005\n",
            "Step 81/1000, Loss: 4.977032661437988, LR: 0.0005\n",
            "Step 82/1000, Loss: 4.973008155822754, LR: 0.0005\n",
            "Step 83/1000, Loss: 4.962676048278809, LR: 0.0005\n",
            "Step 84/1000, Loss: 4.958287239074707, LR: 0.0005\n",
            "Step 85/1000, Loss: 4.937232971191406, LR: 0.0005\n",
            "Step 86/1000, Loss: 4.938880920410156, LR: 0.0005\n",
            "Step 87/1000, Loss: 4.9323883056640625, LR: 0.0005\n",
            "Step 88/1000, Loss: 4.915337085723877, LR: 0.0005\n",
            "Step 89/1000, Loss: 4.907942295074463, LR: 0.0005\n",
            "Step 90/1000, Loss: 4.907235622406006, LR: 0.0005\n",
            "Step 91/1000, Loss: 4.91913366317749, LR: 0.0005\n",
            "Step 92/1000, Loss: 4.912065505981445, LR: 0.0005\n",
            "Step 93/1000, Loss: 4.9071784019470215, LR: 0.0005\n",
            "Step 94/1000, Loss: 4.891799449920654, LR: 0.0005\n",
            "Step 95/1000, Loss: 4.863778114318848, LR: 0.0005\n",
            "Step 96/1000, Loss: 4.856368064880371, LR: 0.0005\n",
            "Step 97/1000, Loss: 4.8463311195373535, LR: 0.0005\n",
            "Step 98/1000, Loss: 4.839901924133301, LR: 0.0005\n",
            "Step 99/1000, Loss: 4.822806358337402, LR: 0.0005\n",
            "Step 100/1000, Loss: 4.818800449371338, LR: 0.0005\n",
            "Step 101/1000, Loss: 4.817400932312012, LR: 0.0005\n",
            "Step 102/1000, Loss: 4.829028129577637, LR: 0.0005\n",
            "Step 103/1000, Loss: 4.808130741119385, LR: 0.0005\n",
            "Step 104/1000, Loss: 4.800762176513672, LR: 0.0005\n",
            "Step 105/1000, Loss: 4.787580490112305, LR: 0.0005\n",
            "Step 106/1000, Loss: 4.774441719055176, LR: 0.0005\n",
            "Step 107/1000, Loss: 4.7720513343811035, LR: 0.0005\n",
            "Step 108/1000, Loss: 4.754755973815918, LR: 0.0005\n",
            "Step 109/1000, Loss: 4.74705696105957, LR: 0.0005\n",
            "Step 110/1000, Loss: 4.745667934417725, LR: 0.0005\n",
            "Step 111/1000, Loss: 4.738838195800781, LR: 0.0005\n",
            "Step 112/1000, Loss: 4.7231574058532715, LR: 0.0005\n",
            "Step 113/1000, Loss: 4.70779275894165, LR: 0.0005\n",
            "Step 114/1000, Loss: 4.6752495765686035, LR: 0.0005\n",
            "Step 115/1000, Loss: 4.653024196624756, LR: 0.0005\n",
            "Step 116/1000, Loss: 4.642952919006348, LR: 0.0005\n",
            "Step 117/1000, Loss: 4.630640983581543, LR: 0.0005\n",
            "Step 118/1000, Loss: 4.6216936111450195, LR: 0.0005\n",
            "Step 119/1000, Loss: 4.608578681945801, LR: 0.0005\n",
            "Step 120/1000, Loss: 4.583082675933838, LR: 0.0005\n",
            "Step 121/1000, Loss: 4.567461967468262, LR: 0.0005\n",
            "Step 122/1000, Loss: 4.533463478088379, LR: 0.0005\n",
            "Step 123/1000, Loss: 4.516829013824463, LR: 0.0005\n",
            "Step 124/1000, Loss: 4.504567623138428, LR: 0.0005\n",
            "Step 125/1000, Loss: 4.474179744720459, LR: 0.0005\n",
            "Step 126/1000, Loss: 4.464488983154297, LR: 0.0005\n",
            "Step 127/1000, Loss: 4.47560977935791, LR: 0.0005\n",
            "Step 128/1000, Loss: 4.4474945068359375, LR: 0.0005\n",
            "Step 129/1000, Loss: 4.437094688415527, LR: 0.0005\n",
            "Step 130/1000, Loss: 4.427935600280762, LR: 0.0005\n",
            "Step 131/1000, Loss: 4.424062252044678, LR: 0.0005\n",
            "Step 132/1000, Loss: 4.39707088470459, LR: 0.0005\n",
            "Step 133/1000, Loss: 4.379190921783447, LR: 0.0005\n",
            "Step 134/1000, Loss: 4.363951206207275, LR: 0.0005\n",
            "Step 135/1000, Loss: 4.3616156578063965, LR: 0.0005\n",
            "Step 136/1000, Loss: 4.369631767272949, LR: 0.0005\n",
            "Step 137/1000, Loss: 4.369208812713623, LR: 0.0005\n",
            "Step 138/1000, Loss: 4.3514604568481445, LR: 0.0005\n",
            "Step 139/1000, Loss: 4.354179859161377, LR: 0.0005\n",
            "Step 140/1000, Loss: 4.329715251922607, LR: 0.0005\n",
            "Step 141/1000, Loss: 4.311864376068115, LR: 0.0005\n",
            "Step 142/1000, Loss: 4.309225559234619, LR: 0.0005\n",
            "Step 143/1000, Loss: 4.301355838775635, LR: 0.0005\n",
            "Step 144/1000, Loss: 4.284416198730469, LR: 0.0005\n",
            "Step 145/1000, Loss: 4.263914585113525, LR: 0.0005\n",
            "Step 146/1000, Loss: 4.262014865875244, LR: 0.0005\n",
            "Step 147/1000, Loss: 4.241598606109619, LR: 0.0005\n",
            "Step 148/1000, Loss: 4.231377601623535, LR: 0.0005\n",
            "Step 149/1000, Loss: 4.21382999420166, LR: 0.0005\n",
            "Step 150/1000, Loss: 4.200963020324707, LR: 0.0005\n",
            "Step 151/1000, Loss: 4.180422306060791, LR: 0.0005\n",
            "Step 152/1000, Loss: 4.167736053466797, LR: 0.0005\n",
            "Step 153/1000, Loss: 4.161160469055176, LR: 0.0005\n",
            "Step 154/1000, Loss: 4.1456618309021, LR: 0.0005\n",
            "Step 155/1000, Loss: 4.152509689331055, LR: 0.0005\n",
            "Step 156/1000, Loss: 4.123193264007568, LR: 0.0005\n",
            "Step 157/1000, Loss: 4.12382173538208, LR: 0.0005\n",
            "Step 158/1000, Loss: 4.113420009613037, LR: 0.0005\n",
            "Step 159/1000, Loss: 4.103420257568359, LR: 0.0005\n",
            "Step 160/1000, Loss: 4.087604522705078, LR: 0.0005\n",
            "Step 161/1000, Loss: 4.064783573150635, LR: 0.0005\n",
            "Step 162/1000, Loss: 4.068674564361572, LR: 0.0005\n",
            "Step 163/1000, Loss: 4.047588348388672, LR: 0.0005\n",
            "Step 164/1000, Loss: 4.045401096343994, LR: 0.0005\n",
            "Step 165/1000, Loss: 4.03665018081665, LR: 0.0005\n",
            "Step 166/1000, Loss: 3.9968700408935547, LR: 0.0005\n",
            "Step 167/1000, Loss: 3.973905086517334, LR: 0.0005\n",
            "Step 168/1000, Loss: 3.9568424224853516, LR: 0.0005\n",
            "Step 169/1000, Loss: 3.9414420127868652, LR: 0.0005\n",
            "Step 170/1000, Loss: 3.924774169921875, LR: 0.0005\n",
            "Step 171/1000, Loss: 3.919344425201416, LR: 0.0005\n",
            "Step 172/1000, Loss: 3.8895328044891357, LR: 0.0005\n",
            "Step 173/1000, Loss: 3.8654937744140625, LR: 0.0005\n",
            "Step 174/1000, Loss: 3.853201389312744, LR: 0.0005\n",
            "Step 175/1000, Loss: 3.8439433574676514, LR: 0.0005\n",
            "Step 176/1000, Loss: 3.8255341053009033, LR: 0.0005\n",
            "Step 177/1000, Loss: 3.8023438453674316, LR: 0.0005\n",
            "Step 178/1000, Loss: 3.780360460281372, LR: 0.0005\n",
            "Step 179/1000, Loss: 3.7734978199005127, LR: 0.0005\n",
            "Step 180/1000, Loss: 3.766247272491455, LR: 0.0005\n",
            "Step 181/1000, Loss: 3.7362022399902344, LR: 0.0005\n",
            "Step 182/1000, Loss: 3.718500852584839, LR: 0.0005\n",
            "Step 183/1000, Loss: 3.7040748596191406, LR: 0.0005\n",
            "Step 184/1000, Loss: 3.6925461292266846, LR: 0.0005\n",
            "Step 185/1000, Loss: 3.7008113861083984, LR: 0.0005\n",
            "Step 186/1000, Loss: 3.681462049484253, LR: 0.0005\n",
            "Step 187/1000, Loss: 3.6597771644592285, LR: 0.0005\n",
            "Step 188/1000, Loss: 3.6474928855895996, LR: 0.0005\n",
            "Step 189/1000, Loss: 3.646015167236328, LR: 0.0005\n",
            "Step 190/1000, Loss: 3.635300397872925, LR: 0.0005\n",
            "Step 191/1000, Loss: 3.6175220012664795, LR: 0.0005\n",
            "Step 192/1000, Loss: 3.601447582244873, LR: 0.0005\n",
            "Step 193/1000, Loss: 3.5824382305145264, LR: 0.0005\n",
            "Step 194/1000, Loss: 3.563476085662842, LR: 0.0005\n",
            "Step 195/1000, Loss: 3.5553271770477295, LR: 0.0005\n",
            "Step 196/1000, Loss: 3.5536751747131348, LR: 0.0005\n",
            "Step 197/1000, Loss: 3.5209968090057373, LR: 0.0005\n",
            "Step 198/1000, Loss: 3.5104317665100098, LR: 0.0005\n",
            "Step 199/1000, Loss: 3.511910915374756, LR: 0.0005\n",
            "Step 200/1000, Loss: 3.499884843826294, LR: 0.0005\n",
            "Step 201/1000, Loss: 3.4979522228240967, LR: 0.0005\n",
            "Step 202/1000, Loss: 3.491229295730591, LR: 0.0005\n",
            "Step 203/1000, Loss: 3.4783248901367188, LR: 0.0005\n",
            "Step 204/1000, Loss: 3.4623024463653564, LR: 0.0005\n",
            "Step 205/1000, Loss: 3.443986415863037, LR: 0.0005\n",
            "Step 206/1000, Loss: 3.431797742843628, LR: 0.0005\n",
            "Step 207/1000, Loss: 3.421011447906494, LR: 0.0005\n",
            "Step 208/1000, Loss: 3.4126694202423096, LR: 0.0005\n",
            "Step 209/1000, Loss: 3.390948534011841, LR: 0.0005\n",
            "Step 210/1000, Loss: 3.3665740489959717, LR: 0.0005\n",
            "Step 211/1000, Loss: 3.3496270179748535, LR: 0.0005\n",
            "Step 212/1000, Loss: 3.3223583698272705, LR: 0.0005\n",
            "Step 213/1000, Loss: 3.3012020587921143, LR: 0.0005\n",
            "Step 214/1000, Loss: 3.2778916358947754, LR: 0.0005\n",
            "Step 215/1000, Loss: 3.263512134552002, LR: 0.0005\n",
            "Step 216/1000, Loss: 3.2511508464813232, LR: 0.0005\n",
            "Step 217/1000, Loss: 3.2278316020965576, LR: 0.0005\n",
            "Step 218/1000, Loss: 3.2092175483703613, LR: 0.0005\n",
            "Step 219/1000, Loss: 3.1759707927703857, LR: 0.0005\n",
            "Step 220/1000, Loss: 3.1664061546325684, LR: 0.0005\n",
            "Step 221/1000, Loss: 3.1475110054016113, LR: 0.0005\n",
            "Step 222/1000, Loss: 3.1363067626953125, LR: 0.0005\n",
            "Step 223/1000, Loss: 3.1176180839538574, LR: 0.0005\n",
            "Step 224/1000, Loss: 3.119788646697998, LR: 0.0005\n",
            "Step 225/1000, Loss: 3.1389591693878174, LR: 0.0005\n",
            "Step 226/1000, Loss: 3.1512084007263184, LR: 0.0005\n",
            "Step 227/1000, Loss: 3.1246891021728516, LR: 0.0005\n",
            "Step 228/1000, Loss: 3.11689829826355, LR: 0.0005\n",
            "Step 229/1000, Loss: 3.1026268005371094, LR: 0.0005\n",
            "Step 230/1000, Loss: 3.0829811096191406, LR: 0.0005\n",
            "Step 231/1000, Loss: 3.0686793327331543, LR: 0.0005\n",
            "Step 232/1000, Loss: 3.0719714164733887, LR: 0.0005\n",
            "Step 233/1000, Loss: 3.060706853866577, LR: 0.0005\n",
            "Step 234/1000, Loss: 3.027188777923584, LR: 0.0005\n",
            "Step 235/1000, Loss: 3.0117859840393066, LR: 0.0005\n",
            "Step 236/1000, Loss: 2.998744249343872, LR: 0.0005\n",
            "Step 237/1000, Loss: 3.0093624591827393, LR: 0.0005\n",
            "Step 238/1000, Loss: 2.982064962387085, LR: 0.0005\n",
            "Step 239/1000, Loss: 2.971196174621582, LR: 0.0005\n",
            "Step 240/1000, Loss: 2.947056293487549, LR: 0.0005\n",
            "Step 241/1000, Loss: 2.92566180229187, LR: 0.0005\n",
            "Step 242/1000, Loss: 2.896009922027588, LR: 0.0005\n",
            "Step 243/1000, Loss: 2.884242296218872, LR: 0.0005\n",
            "Step 244/1000, Loss: 2.8650569915771484, LR: 0.0005\n",
            "Step 245/1000, Loss: 2.8416054248809814, LR: 0.0005\n",
            "Step 246/1000, Loss: 2.821000337600708, LR: 0.0005\n",
            "Step 247/1000, Loss: 2.798201084136963, LR: 0.0005\n",
            "Step 248/1000, Loss: 2.7995715141296387, LR: 0.0005\n",
            "Step 249/1000, Loss: 2.793076276779175, LR: 0.0005\n",
            "Step 250/1000, Loss: 2.7850966453552246, LR: 0.0005\n",
            "Step 251/1000, Loss: 2.7532291412353516, LR: 0.0005\n",
            "Step 252/1000, Loss: 2.7132554054260254, LR: 0.0005\n",
            "Step 253/1000, Loss: 2.685918092727661, LR: 0.0005\n",
            "Step 254/1000, Loss: 2.65861177444458, LR: 0.0005\n",
            "Step 255/1000, Loss: 2.6585001945495605, LR: 0.0005\n",
            "Step 256/1000, Loss: 2.643608570098877, LR: 0.0005\n",
            "Step 257/1000, Loss: 2.631662368774414, LR: 0.0005\n",
            "Step 258/1000, Loss: 2.6338000297546387, LR: 0.0005\n",
            "Step 259/1000, Loss: 2.609696865081787, LR: 0.0005\n",
            "Step 260/1000, Loss: 2.6071720123291016, LR: 0.0005\n",
            "Step 261/1000, Loss: 2.613776683807373, LR: 0.0005\n",
            "Step 262/1000, Loss: 2.6126132011413574, LR: 0.0005\n",
            "Step 263/1000, Loss: 2.6090948581695557, LR: 0.0005\n",
            "Step 264/1000, Loss: 2.5916454792022705, LR: 0.0005\n",
            "Step 265/1000, Loss: 2.5592198371887207, LR: 0.0005\n",
            "Step 266/1000, Loss: 2.5549540519714355, LR: 0.0005\n",
            "Step 267/1000, Loss: 2.5567924976348877, LR: 0.0005\n",
            "Step 268/1000, Loss: 2.5651726722717285, LR: 0.0005\n",
            "Step 269/1000, Loss: 2.562312602996826, LR: 0.0005\n",
            "Step 270/1000, Loss: 2.5357327461242676, LR: 0.0005\n",
            "Step 271/1000, Loss: 2.531230926513672, LR: 0.0005\n",
            "Step 272/1000, Loss: 2.521047592163086, LR: 0.0005\n",
            "Step 273/1000, Loss: 2.500943183898926, LR: 0.0005\n",
            "Step 274/1000, Loss: 2.4659016132354736, LR: 0.0005\n",
            "Step 275/1000, Loss: 2.4308254718780518, LR: 0.0005\n",
            "Step 276/1000, Loss: 2.4142773151397705, LR: 0.0005\n",
            "Step 277/1000, Loss: 2.4153549671173096, LR: 0.0005\n",
            "Step 278/1000, Loss: 2.4024853706359863, LR: 0.0005\n",
            "Step 279/1000, Loss: 2.389504909515381, LR: 0.0005\n",
            "Step 280/1000, Loss: 2.406141519546509, LR: 0.0005\n",
            "Step 281/1000, Loss: 2.3784077167510986, LR: 0.0005\n",
            "Step 282/1000, Loss: 2.3545751571655273, LR: 0.0005\n",
            "Step 283/1000, Loss: 2.3419597148895264, LR: 0.0005\n",
            "Step 284/1000, Loss: 2.3110461235046387, LR: 0.0005\n",
            "Step 285/1000, Loss: 2.296950101852417, LR: 0.0005\n",
            "Step 286/1000, Loss: 2.2750868797302246, LR: 0.0005\n",
            "Step 287/1000, Loss: 2.260720729827881, LR: 0.0005\n",
            "Step 288/1000, Loss: 2.234762668609619, LR: 0.0005\n",
            "Step 289/1000, Loss: 2.238283157348633, LR: 0.0005\n",
            "Step 290/1000, Loss: 2.2176032066345215, LR: 0.0005\n",
            "Step 291/1000, Loss: 2.2396597862243652, LR: 0.0005\n",
            "Step 292/1000, Loss: 2.230252742767334, LR: 0.0005\n",
            "Step 293/1000, Loss: 2.219910144805908, LR: 0.0005\n",
            "Step 294/1000, Loss: 2.2257516384124756, LR: 0.0005\n",
            "Step 295/1000, Loss: 2.222249746322632, LR: 0.0005\n",
            "Step 296/1000, Loss: 2.18711519241333, LR: 0.0005\n",
            "Step 297/1000, Loss: 2.164632558822632, LR: 0.0005\n",
            "Step 298/1000, Loss: 2.1307711601257324, LR: 0.0005\n",
            "Step 299/1000, Loss: 2.119112730026245, LR: 0.0005\n",
            "Step 300/1000, Loss: 2.095324993133545, LR: 0.0005\n",
            "Step 301/1000, Loss: 2.0848896503448486, LR: 0.0005\n",
            "Step 302/1000, Loss: 2.058800220489502, LR: 0.0005\n",
            "Step 303/1000, Loss: 2.026474714279175, LR: 0.0005\n",
            "Step 304/1000, Loss: 2.0039222240448, LR: 0.0005\n",
            "Step 305/1000, Loss: 2.016700029373169, LR: 0.0005\n",
            "Step 306/1000, Loss: 2.0043187141418457, LR: 0.0005\n",
            "Step 307/1000, Loss: 1.9794483184814453, LR: 0.0005\n",
            "Step 308/1000, Loss: 1.9760795831680298, LR: 0.0005\n",
            "Step 309/1000, Loss: 1.9539073705673218, LR: 0.0005\n",
            "Step 310/1000, Loss: 1.9415565729141235, LR: 0.0005\n",
            "Step 311/1000, Loss: 1.915337324142456, LR: 0.0005\n",
            "Step 312/1000, Loss: 1.8846263885498047, LR: 0.0005\n",
            "Step 313/1000, Loss: 1.8885055780410767, LR: 0.0005\n",
            "Step 314/1000, Loss: 1.9001178741455078, LR: 0.0005\n",
            "Step 315/1000, Loss: 1.9070762395858765, LR: 0.0005\n",
            "Step 316/1000, Loss: 1.8783056735992432, LR: 0.0005\n",
            "Step 317/1000, Loss: 1.8867132663726807, LR: 0.0005\n",
            "Step 318/1000, Loss: 1.8637596368789673, LR: 0.0005\n",
            "Step 319/1000, Loss: 1.8269727230072021, LR: 0.0005\n",
            "Step 320/1000, Loss: 1.824231743812561, LR: 0.0005\n",
            "Step 321/1000, Loss: 1.7930679321289062, LR: 0.0005\n",
            "Step 322/1000, Loss: 1.774756669998169, LR: 0.0005\n",
            "Step 323/1000, Loss: 1.7638578414916992, LR: 0.0005\n",
            "Step 324/1000, Loss: 1.7341969013214111, LR: 0.0005\n",
            "Step 325/1000, Loss: 1.7099231481552124, LR: 0.0005\n",
            "Step 326/1000, Loss: 1.6952135562896729, LR: 0.0005\n",
            "Step 327/1000, Loss: 1.6680958271026611, LR: 0.0005\n",
            "Step 328/1000, Loss: 1.634503960609436, LR: 0.0005\n",
            "Step 329/1000, Loss: 1.6364991664886475, LR: 0.0005\n",
            "Step 330/1000, Loss: 1.6475017070770264, LR: 0.0005\n",
            "Step 331/1000, Loss: 1.6339340209960938, LR: 0.0005\n",
            "Step 332/1000, Loss: 1.6181182861328125, LR: 0.0005\n",
            "Step 333/1000, Loss: 1.594831109046936, LR: 0.0005\n",
            "Step 334/1000, Loss: 1.590966820716858, LR: 0.0005\n",
            "Step 335/1000, Loss: 1.573487639427185, LR: 0.0005\n",
            "Step 336/1000, Loss: 1.5511974096298218, LR: 0.0005\n",
            "Step 337/1000, Loss: 1.543217420578003, LR: 0.0005\n",
            "Step 338/1000, Loss: 1.5490005016326904, LR: 0.0005\n",
            "Step 339/1000, Loss: 1.5397249460220337, LR: 0.0005\n",
            "Step 340/1000, Loss: 1.522788405418396, LR: 0.0005\n",
            "Step 341/1000, Loss: 1.5455958843231201, LR: 0.0005\n",
            "Step 342/1000, Loss: 1.5338735580444336, LR: 0.0005\n",
            "Step 343/1000, Loss: 1.530958890914917, LR: 0.0005\n",
            "Step 344/1000, Loss: 1.5448294878005981, LR: 0.0005\n",
            "Step 345/1000, Loss: 1.511346459388733, LR: 0.0005\n",
            "Step 346/1000, Loss: 1.490561842918396, LR: 0.0005\n",
            "Step 347/1000, Loss: 1.4749597311019897, LR: 0.0005\n",
            "Step 348/1000, Loss: 1.447224497795105, LR: 0.0005\n",
            "Step 349/1000, Loss: 1.4411523342132568, LR: 0.0005\n",
            "Step 350/1000, Loss: 1.4102349281311035, LR: 0.0005\n",
            "Step 351/1000, Loss: 1.380233883857727, LR: 0.0005\n",
            "Step 352/1000, Loss: 1.3802143335342407, LR: 0.0005\n",
            "Step 353/1000, Loss: 1.39609694480896, LR: 0.0005\n",
            "Step 354/1000, Loss: 1.3913711309432983, LR: 0.0005\n",
            "Step 355/1000, Loss: 1.376795768737793, LR: 0.0005\n",
            "Step 356/1000, Loss: 1.3834755420684814, LR: 0.0005\n",
            "Step 357/1000, Loss: 1.3637926578521729, LR: 0.0005\n",
            "Step 358/1000, Loss: 1.3437178134918213, LR: 0.0005\n",
            "Step 359/1000, Loss: 1.32669198513031, LR: 0.0005\n",
            "Step 360/1000, Loss: 1.3122389316558838, LR: 0.0005\n",
            "Step 361/1000, Loss: 1.304546594619751, LR: 0.0005\n",
            "Step 362/1000, Loss: 1.274172067642212, LR: 0.0005\n",
            "Step 363/1000, Loss: 1.260986566543579, LR: 0.0005\n",
            "Step 364/1000, Loss: 1.235002875328064, LR: 0.0005\n",
            "Step 365/1000, Loss: 1.201599359512329, LR: 0.0005\n",
            "Step 366/1000, Loss: 1.1657415628433228, LR: 0.0005\n",
            "Step 367/1000, Loss: 1.1613693237304688, LR: 0.0005\n",
            "Step 368/1000, Loss: 1.138730764389038, LR: 0.0005\n",
            "Step 369/1000, Loss: 1.1358407735824585, LR: 0.0005\n",
            "Step 370/1000, Loss: 1.1541863679885864, LR: 0.0005\n",
            "Step 371/1000, Loss: 1.1090123653411865, LR: 0.0005\n",
            "Step 372/1000, Loss: 1.1131991147994995, LR: 0.0005\n",
            "Step 373/1000, Loss: 1.0961244106292725, LR: 0.0005\n",
            "Step 374/1000, Loss: 1.1035740375518799, LR: 0.0005\n",
            "Step 375/1000, Loss: 1.0620877742767334, LR: 0.0005\n",
            "Step 376/1000, Loss: 1.0569641590118408, LR: 0.0005\n",
            "Step 377/1000, Loss: 1.0546218156814575, LR: 0.0005\n",
            "Step 378/1000, Loss: 1.0408533811569214, LR: 0.0005\n",
            "Step 379/1000, Loss: 1.0240854024887085, LR: 0.0005\n",
            "Step 380/1000, Loss: 0.9987276196479797, LR: 0.0005\n",
            "Step 381/1000, Loss: 1.0030957460403442, LR: 0.0005\n",
            "Step 382/1000, Loss: 0.9741361737251282, LR: 0.0005\n",
            "Step 383/1000, Loss: 0.9790817499160767, LR: 0.0005\n",
            "Step 384/1000, Loss: 0.978474497795105, LR: 0.0005\n",
            "Step 385/1000, Loss: 0.9776767492294312, LR: 0.0005\n",
            "Step 386/1000, Loss: 0.9669596552848816, LR: 0.0005\n",
            "Step 387/1000, Loss: 0.9331640005111694, LR: 0.0005\n",
            "Step 388/1000, Loss: 0.9237254858016968, LR: 0.0005\n",
            "Step 389/1000, Loss: 0.9135242700576782, LR: 0.0005\n",
            "Step 390/1000, Loss: 0.8878000378608704, LR: 0.0005\n",
            "Step 391/1000, Loss: 0.8806230425834656, LR: 0.0005\n",
            "Step 392/1000, Loss: 0.8629080057144165, LR: 0.0005\n",
            "Step 393/1000, Loss: 0.8426729440689087, LR: 0.0005\n",
            "Step 394/1000, Loss: 0.8433264493942261, LR: 0.0005\n",
            "Step 395/1000, Loss: 0.8290466070175171, LR: 0.0005\n",
            "Step 396/1000, Loss: 0.8209354281425476, LR: 0.0005\n",
            "Step 397/1000, Loss: 0.7996379733085632, LR: 0.0005\n",
            "Step 398/1000, Loss: 0.8010199666023254, LR: 0.0005\n",
            "Step 399/1000, Loss: 0.785988450050354, LR: 0.0005\n",
            "Step 400/1000, Loss: 0.7650213241577148, LR: 0.0005\n",
            "Step 401/1000, Loss: 0.7508553266525269, LR: 0.0005\n",
            "Step 402/1000, Loss: 0.7320271134376526, LR: 0.0005\n",
            "Step 403/1000, Loss: 0.710856556892395, LR: 0.0005\n",
            "Step 404/1000, Loss: 0.7207274436950684, LR: 0.0005\n",
            "Step 405/1000, Loss: 0.7083617448806763, LR: 0.0005\n",
            "Step 406/1000, Loss: 0.7027336359024048, LR: 0.0005\n",
            "Step 407/1000, Loss: 0.7078672051429749, LR: 0.0005\n",
            "Step 408/1000, Loss: 0.7094935178756714, LR: 0.0005\n",
            "Step 409/1000, Loss: 0.7054926156997681, LR: 0.0005\n",
            "Step 410/1000, Loss: 0.6799963712692261, LR: 0.0005\n",
            "Step 411/1000, Loss: 0.6812117695808411, LR: 0.0005\n",
            "Step 412/1000, Loss: 0.6730329394340515, LR: 0.0005\n",
            "Step 413/1000, Loss: 0.6743412017822266, LR: 0.0005\n",
            "Step 414/1000, Loss: 0.6654385924339294, LR: 0.0005\n",
            "Step 415/1000, Loss: 0.6566230058670044, LR: 0.0005\n",
            "Step 416/1000, Loss: 0.6562038660049438, LR: 0.0005\n",
            "Step 417/1000, Loss: 0.6455215215682983, LR: 0.0005\n",
            "Step 418/1000, Loss: 0.6123485565185547, LR: 0.0005\n",
            "Step 419/1000, Loss: 0.6105880737304688, LR: 0.0005\n",
            "Step 420/1000, Loss: 0.5989521741867065, LR: 0.0005\n",
            "Step 421/1000, Loss: 0.5776963233947754, LR: 0.0005\n",
            "Step 422/1000, Loss: 0.5917723774909973, LR: 0.0005\n",
            "Step 423/1000, Loss: 0.5826953053474426, LR: 0.0005\n",
            "Step 424/1000, Loss: 0.5759533047676086, LR: 0.0005\n",
            "Step 425/1000, Loss: 0.5631542801856995, LR: 0.0005\n",
            "Step 426/1000, Loss: 0.5591896772384644, LR: 0.0005\n",
            "Step 427/1000, Loss: 0.5417264699935913, LR: 0.0005\n",
            "Step 428/1000, Loss: 0.5381447672843933, LR: 0.0005\n",
            "Step 429/1000, Loss: 0.5417950749397278, LR: 0.0005\n",
            "Step 430/1000, Loss: 0.5303422808647156, LR: 0.0005\n",
            "Step 431/1000, Loss: 0.5049916505813599, LR: 0.0005\n",
            "Step 432/1000, Loss: 0.5250224471092224, LR: 0.0005\n",
            "Step 433/1000, Loss: 0.516842246055603, LR: 0.0005\n",
            "Step 434/1000, Loss: 0.518926203250885, LR: 0.0005\n",
            "Step 435/1000, Loss: 0.5102407336235046, LR: 0.0005\n",
            "Step 436/1000, Loss: 0.5253453254699707, LR: 0.0005\n",
            "Step 437/1000, Loss: 0.527912974357605, LR: 0.0005\n",
            "Step 438/1000, Loss: 0.5067081451416016, LR: 0.0005\n",
            "Step 439/1000, Loss: 0.5012630820274353, LR: 0.0005\n",
            "Step 440/1000, Loss: 0.4986436367034912, LR: 0.0005\n",
            "Step 441/1000, Loss: 0.4857131838798523, LR: 0.0005\n",
            "Step 442/1000, Loss: 0.4838215410709381, LR: 0.0005\n",
            "Step 443/1000, Loss: 0.4898408055305481, LR: 0.0005\n",
            "Step 444/1000, Loss: 0.4877270758152008, LR: 0.0005\n",
            "Step 445/1000, Loss: 0.47187289595603943, LR: 0.0005\n",
            "Step 446/1000, Loss: 0.49167880415916443, LR: 0.0005\n",
            "Step 447/1000, Loss: 0.4904972016811371, LR: 0.0005\n",
            "Step 448/1000, Loss: 0.48539894819259644, LR: 0.0005\n",
            "Step 449/1000, Loss: 0.48002782464027405, LR: 0.0005\n",
            "Step 450/1000, Loss: 0.47746285796165466, LR: 0.0005\n",
            "Step 451/1000, Loss: 0.4799131453037262, LR: 0.0005\n",
            "Step 452/1000, Loss: 0.4878813624382019, LR: 0.0005\n",
            "Step 453/1000, Loss: 0.5221856236457825, LR: 0.0005\n",
            "Step 454/1000, Loss: 0.5054720044136047, LR: 0.0005\n",
            "Step 455/1000, Loss: 0.482340008020401, LR: 0.0005\n",
            "Step 456/1000, Loss: 0.4927099347114563, LR: 0.0005\n",
            "Step 457/1000, Loss: 0.4957008361816406, LR: 0.0005\n",
            "Step 458/1000, Loss: 0.4730059504508972, LR: 0.0005\n",
            "Step 459/1000, Loss: 0.45871466398239136, LR: 0.0005\n",
            "Step 460/1000, Loss: 0.4534568190574646, LR: 0.0005\n",
            "Step 461/1000, Loss: 0.4672177731990814, LR: 0.0005\n",
            "Step 462/1000, Loss: 0.45965448021888733, LR: 0.0005\n",
            "Step 463/1000, Loss: 0.44656437635421753, LR: 0.0005\n",
            "Step 464/1000, Loss: 0.43963518738746643, LR: 0.0005\n",
            "Step 465/1000, Loss: 0.4178427755832672, LR: 0.0005\n",
            "Step 466/1000, Loss: 0.41813191771507263, LR: 0.0005\n",
            "Step 467/1000, Loss: 0.4158242344856262, LR: 0.0005\n",
            "Step 468/1000, Loss: 0.40678268671035767, LR: 0.0005\n",
            "Step 469/1000, Loss: 0.4039410650730133, LR: 0.0005\n",
            "Step 470/1000, Loss: 0.42513567209243774, LR: 0.0005\n",
            "Step 471/1000, Loss: 0.42674025893211365, LR: 0.0005\n",
            "Step 472/1000, Loss: 0.40094202756881714, LR: 0.0005\n",
            "Step 473/1000, Loss: 0.3931654393672943, LR: 0.0005\n",
            "Step 474/1000, Loss: 0.38468465209007263, LR: 0.0005\n",
            "Step 475/1000, Loss: 0.3704591393470764, LR: 0.0005\n",
            "Step 476/1000, Loss: 0.375814288854599, LR: 0.0005\n",
            "Step 477/1000, Loss: 0.36047664284706116, LR: 0.0005\n",
            "Step 478/1000, Loss: 0.35861530900001526, LR: 0.0005\n",
            "Step 479/1000, Loss: 0.3694741129875183, LR: 0.0005\n",
            "Step 480/1000, Loss: 0.3435516357421875, LR: 0.0005\n",
            "Step 481/1000, Loss: 0.3459782600402832, LR: 0.0005\n",
            "Step 482/1000, Loss: 0.34499722719192505, LR: 0.0005\n",
            "Step 483/1000, Loss: 0.3355053663253784, LR: 0.0005\n",
            "Step 484/1000, Loss: 0.32276371121406555, LR: 0.0005\n",
            "Step 485/1000, Loss: 0.31779956817626953, LR: 0.0005\n",
            "Step 486/1000, Loss: 0.3160385489463806, LR: 0.0005\n",
            "Step 487/1000, Loss: 0.31299272179603577, LR: 0.0005\n",
            "Step 488/1000, Loss: 0.32064104080200195, LR: 0.0005\n",
            "Step 489/1000, Loss: 0.31816571950912476, LR: 0.0005\n",
            "Step 490/1000, Loss: 0.3205655515193939, LR: 0.0005\n",
            "Step 491/1000, Loss: 0.31671541929244995, LR: 0.0005\n",
            "Step 492/1000, Loss: 0.316073477268219, LR: 0.0005\n",
            "Step 493/1000, Loss: 0.3078405261039734, LR: 0.0005\n",
            "Step 494/1000, Loss: 0.30757659673690796, LR: 0.0005\n",
            "Step 495/1000, Loss: 0.29420632123947144, LR: 0.0005\n",
            "Step 496/1000, Loss: 0.2836211323738098, LR: 0.0005\n",
            "Step 497/1000, Loss: 0.2972412109375, LR: 0.0005\n",
            "Step 498/1000, Loss: 0.2987685203552246, LR: 0.0005\n",
            "Step 499/1000, Loss: 0.28979387879371643, LR: 0.0005\n",
            "Step 500/1000, Loss: 0.3017769753932953, LR: 0.0005\n",
            "Step 501/1000, Loss: 0.28742948174476624, LR: 0.0005\n",
            "Step 502/1000, Loss: 0.2765437066555023, LR: 0.0005\n",
            "Step 503/1000, Loss: 0.26422515511512756, LR: 0.0005\n",
            "Step 504/1000, Loss: 0.27131420373916626, LR: 0.0005\n",
            "Step 505/1000, Loss: 0.27844205498695374, LR: 0.0005\n",
            "Step 506/1000, Loss: 0.2561343312263489, LR: 0.0005\n",
            "Step 507/1000, Loss: 0.2618893086910248, LR: 0.0005\n",
            "Step 508/1000, Loss: 0.2711230218410492, LR: 0.0005\n",
            "Step 509/1000, Loss: 0.27242523431777954, LR: 0.0005\n",
            "Step 510/1000, Loss: 0.26488161087036133, LR: 0.0005\n",
            "Step 511/1000, Loss: 0.2691825032234192, LR: 0.0005\n",
            "Step 512/1000, Loss: 0.26107728481292725, LR: 0.0005\n",
            "Step 513/1000, Loss: 0.2676060199737549, LR: 0.0005\n",
            "Step 514/1000, Loss: 0.2535170018672943, LR: 0.0005\n",
            "Step 515/1000, Loss: 0.2448393553495407, LR: 0.0005\n",
            "Step 516/1000, Loss: 0.23578190803527832, LR: 0.0005\n",
            "Step 517/1000, Loss: 0.22918491065502167, LR: 0.0005\n",
            "Step 518/1000, Loss: 0.22660085558891296, LR: 0.0005\n",
            "Step 519/1000, Loss: 0.22256752848625183, LR: 0.0005\n",
            "Step 520/1000, Loss: 0.22815687954425812, LR: 0.0005\n",
            "Step 521/1000, Loss: 0.23004508018493652, LR: 0.0005\n",
            "Step 522/1000, Loss: 0.2423570156097412, LR: 0.0005\n",
            "Step 523/1000, Loss: 0.23528799414634705, LR: 0.0005\n",
            "Step 524/1000, Loss: 0.21101751923561096, LR: 0.0005\n",
            "Step 525/1000, Loss: 0.2106734961271286, LR: 0.0005\n",
            "Step 526/1000, Loss: 0.20905384421348572, LR: 0.0005\n",
            "Step 527/1000, Loss: 0.20081034302711487, LR: 0.0005\n",
            "Step 528/1000, Loss: 0.20484204590320587, LR: 0.0005\n",
            "Step 529/1000, Loss: 0.20058615505695343, LR: 0.0005\n",
            "Step 530/1000, Loss: 0.19470998644828796, LR: 0.0005\n",
            "Step 531/1000, Loss: 0.19290074706077576, LR: 0.0005\n",
            "Step 532/1000, Loss: 0.1976926028728485, LR: 0.0005\n",
            "Step 533/1000, Loss: 0.2059103548526764, LR: 0.0005\n",
            "Step 534/1000, Loss: 0.20744633674621582, LR: 0.0005\n",
            "Step 535/1000, Loss: 0.21628940105438232, LR: 0.0005\n",
            "Step 536/1000, Loss: 0.22242729365825653, LR: 0.0005\n",
            "Step 537/1000, Loss: 0.2281755954027176, LR: 0.0005\n",
            "Step 538/1000, Loss: 0.2202131748199463, LR: 0.0005\n",
            "Step 539/1000, Loss: 0.2060691863298416, LR: 0.0005\n",
            "Step 540/1000, Loss: 0.20827224850654602, LR: 0.0005\n",
            "Step 541/1000, Loss: 0.19638948142528534, LR: 0.0005\n",
            "Step 542/1000, Loss: 0.1844201236963272, LR: 0.0005\n",
            "Step 543/1000, Loss: 0.1926141083240509, LR: 0.0005\n",
            "Step 544/1000, Loss: 0.20242032408714294, LR: 0.0005\n",
            "Step 545/1000, Loss: 0.2121889293193817, LR: 0.0005\n",
            "Step 546/1000, Loss: 0.2009742707014084, LR: 0.0005\n",
            "Step 547/1000, Loss: 0.20131048560142517, LR: 0.0005\n",
            "Step 548/1000, Loss: 0.19917407631874084, LR: 0.0005\n",
            "Step 549/1000, Loss: 0.20642724633216858, LR: 0.0005\n",
            "Step 550/1000, Loss: 0.20181803405284882, LR: 0.0005\n",
            "Step 551/1000, Loss: 0.18122605979442596, LR: 0.0005\n",
            "Step 552/1000, Loss: 0.19900411367416382, LR: 0.0005\n",
            "Step 553/1000, Loss: 0.19747385382652283, LR: 0.0005\n",
            "Step 554/1000, Loss: 0.17542096972465515, LR: 0.0005\n",
            "Step 555/1000, Loss: 0.18866196274757385, LR: 0.0005\n",
            "Step 556/1000, Loss: 0.19373901188373566, LR: 0.0005\n",
            "Step 557/1000, Loss: 0.17430046200752258, LR: 0.0005\n",
            "Step 558/1000, Loss: 0.181413933634758, LR: 0.0005\n",
            "Step 559/1000, Loss: 0.17228253185749054, LR: 0.0005\n",
            "Step 560/1000, Loss: 0.16489644348621368, LR: 0.0005\n",
            "Step 561/1000, Loss: 0.16422414779663086, LR: 0.0005\n",
            "Step 562/1000, Loss: 0.1735035628080368, LR: 0.0005\n",
            "Step 563/1000, Loss: 0.16641931235790253, LR: 0.0005\n",
            "Step 564/1000, Loss: 0.1711777001619339, LR: 0.0005\n",
            "Step 565/1000, Loss: 0.16730733215808868, LR: 0.0005\n",
            "Step 566/1000, Loss: 0.16816917061805725, LR: 0.0005\n",
            "Step 567/1000, Loss: 0.18830648064613342, LR: 0.0005\n",
            "Step 568/1000, Loss: 0.18853513896465302, LR: 0.0005\n",
            "Step 569/1000, Loss: 0.18546952307224274, LR: 0.0005\n",
            "Step 570/1000, Loss: 0.1824268102645874, LR: 0.0005\n",
            "Step 571/1000, Loss: 0.1734406054019928, LR: 0.0005\n",
            "Step 572/1000, Loss: 0.17103545367717743, LR: 0.0005\n",
            "Step 573/1000, Loss: 0.1804053783416748, LR: 0.0005\n",
            "Step 574/1000, Loss: 0.1770731508731842, LR: 0.0005\n",
            "Step 575/1000, Loss: 0.18162593245506287, LR: 0.0005\n",
            "Step 576/1000, Loss: 0.1730504333972931, LR: 0.0005\n",
            "Step 577/1000, Loss: 0.17170806229114532, LR: 0.0005\n",
            "Step 578/1000, Loss: 0.16684941947460175, LR: 0.0005\n",
            "Step 579/1000, Loss: 0.1658213883638382, LR: 0.0005\n",
            "Step 580/1000, Loss: 0.15179142355918884, LR: 0.0005\n",
            "Step 581/1000, Loss: 0.1510123461484909, LR: 0.0005\n",
            "Step 582/1000, Loss: 0.14934685826301575, LR: 0.0005\n",
            "Step 583/1000, Loss: 0.15561406314373016, LR: 0.0005\n",
            "Step 584/1000, Loss: 0.1505437195301056, LR: 0.0005\n",
            "Step 585/1000, Loss: 0.13666248321533203, LR: 0.0005\n",
            "Step 586/1000, Loss: 0.12670156359672546, LR: 0.0005\n",
            "Step 587/1000, Loss: 0.12618203461170197, LR: 0.0005\n",
            "Step 588/1000, Loss: 0.13628630340099335, LR: 0.0005\n",
            "Step 589/1000, Loss: 0.12725000083446503, LR: 0.0005\n",
            "Step 590/1000, Loss: 0.12712474167346954, LR: 0.0005\n",
            "Step 591/1000, Loss: 0.12707200646400452, LR: 0.0005\n",
            "Step 592/1000, Loss: 0.13957403600215912, LR: 0.0005\n",
            "Step 593/1000, Loss: 0.12960870563983917, LR: 0.0005\n",
            "Step 594/1000, Loss: 0.12259743362665176, LR: 0.0005\n",
            "Step 595/1000, Loss: 0.12575244903564453, LR: 0.0005\n",
            "Step 596/1000, Loss: 0.13802213966846466, LR: 0.0005\n",
            "Step 597/1000, Loss: 0.1308528184890747, LR: 0.0005\n",
            "Step 598/1000, Loss: 0.12661036849021912, LR: 0.0005\n",
            "Step 599/1000, Loss: 0.13763917982578278, LR: 0.0005\n",
            "Step 600/1000, Loss: 0.14394262433052063, LR: 0.0005\n",
            "Step 601/1000, Loss: 0.13188931345939636, LR: 0.0005\n",
            "Step 602/1000, Loss: 0.1290334016084671, LR: 0.0005\n",
            "Step 603/1000, Loss: 0.12598587572574615, LR: 0.0005\n",
            "Step 604/1000, Loss: 0.12355464696884155, LR: 0.0005\n",
            "Step 605/1000, Loss: 0.11987967789173126, LR: 0.0005\n",
            "Step 606/1000, Loss: 0.1331193447113037, LR: 0.0005\n",
            "Step 607/1000, Loss: 0.12153427302837372, LR: 0.0005\n",
            "Step 608/1000, Loss: 0.12517130374908447, LR: 0.0005\n",
            "Step 609/1000, Loss: 0.12810957431793213, LR: 0.0005\n",
            "Step 610/1000, Loss: 0.1239907518029213, LR: 0.0005\n",
            "Step 611/1000, Loss: 0.11239363998174667, LR: 0.0005\n",
            "Step 612/1000, Loss: 0.10888508707284927, LR: 0.0005\n",
            "Step 613/1000, Loss: 0.10225912183523178, LR: 0.0005\n",
            "Step 614/1000, Loss: 0.10391690582036972, LR: 0.0005\n",
            "Step 615/1000, Loss: 0.09791199117898941, LR: 0.0005\n",
            "Step 616/1000, Loss: 0.09479818493127823, LR: 0.0005\n",
            "Step 617/1000, Loss: 0.10448092222213745, LR: 0.0005\n",
            "Step 618/1000, Loss: 0.11675498634576797, LR: 0.0005\n",
            "Step 619/1000, Loss: 0.1038195863366127, LR: 0.0005\n",
            "Step 620/1000, Loss: 0.10228829085826874, LR: 0.0005\n",
            "Step 621/1000, Loss: 0.11428888142108917, LR: 0.0005\n",
            "Step 622/1000, Loss: 0.11079506576061249, LR: 0.0005\n",
            "Step 623/1000, Loss: 0.10847431421279907, LR: 0.0005\n",
            "Step 624/1000, Loss: 0.11951317638158798, LR: 0.0005\n",
            "Step 625/1000, Loss: 0.14106735587120056, LR: 0.0005\n",
            "Step 626/1000, Loss: 0.12765195965766907, LR: 0.0005\n",
            "Step 627/1000, Loss: 0.12914793193340302, LR: 0.0005\n",
            "Step 628/1000, Loss: 0.1230907067656517, LR: 0.0005\n",
            "Step 629/1000, Loss: 0.1166960820555687, LR: 0.0005\n",
            "Step 630/1000, Loss: 0.11566338688135147, LR: 0.0005\n",
            "Step 631/1000, Loss: 0.12277895212173462, LR: 0.0005\n",
            "Step 632/1000, Loss: 0.10537320375442505, LR: 0.0005\n",
            "Step 633/1000, Loss: 0.11214794963598251, LR: 0.0005\n",
            "Step 634/1000, Loss: 0.10641114413738251, LR: 0.0005\n",
            "Step 635/1000, Loss: 0.10656313598155975, LR: 0.0005\n",
            "Step 636/1000, Loss: 0.11265920102596283, LR: 0.0005\n",
            "Step 637/1000, Loss: 0.11519172042608261, LR: 0.0001\n",
            "Step 638/1000, Loss: 0.11341552436351776, LR: 0.0001\n",
            "Step 639/1000, Loss: 0.108054019510746, LR: 0.0001\n",
            "Step 640/1000, Loss: 0.10274412482976913, LR: 0.0001\n",
            "Step 641/1000, Loss: 0.0993715301156044, LR: 0.0001\n",
            "Step 642/1000, Loss: 0.09544876962900162, LR: 0.0001\n",
            "Step 643/1000, Loss: 0.09123045951128006, LR: 0.0001\n",
            "Step 644/1000, Loss: 0.08843521773815155, LR: 0.0001\n",
            "Step 645/1000, Loss: 0.0843082144856453, LR: 0.0001\n",
            "Step 646/1000, Loss: 0.08251014351844788, LR: 0.0001\n",
            "Step 647/1000, Loss: 0.07752960175275803, LR: 0.0001\n",
            "Step 648/1000, Loss: 0.07532818615436554, LR: 0.0001\n",
            "Step 649/1000, Loss: 0.0720234364271164, LR: 0.0001\n",
            "Step 650/1000, Loss: 0.06932206451892853, LR: 0.0001\n",
            "Step 651/1000, Loss: 0.06647966802120209, LR: 0.0001\n",
            "Step 652/1000, Loss: 0.06328928470611572, LR: 0.0001\n",
            "Step 653/1000, Loss: 0.062459517270326614, LR: 0.0001\n",
            "Step 654/1000, Loss: 0.060574568808078766, LR: 0.0001\n",
            "Step 655/1000, Loss: 0.05782579258084297, LR: 0.0001\n",
            "Step 656/1000, Loss: 0.05753573030233383, LR: 0.0001\n",
            "Step 657/1000, Loss: 0.05671496316790581, LR: 0.0001\n",
            "Step 658/1000, Loss: 0.05333521217107773, LR: 0.0001\n",
            "Step 659/1000, Loss: 0.052461761981248856, LR: 0.0001\n",
            "Step 660/1000, Loss: 0.05087929964065552, LR: 0.0001\n",
            "Step 661/1000, Loss: 0.05029473453760147, LR: 0.0001\n",
            "Step 662/1000, Loss: 0.04785618185997009, LR: 0.0001\n",
            "Step 663/1000, Loss: 0.046829305589199066, LR: 0.0001\n",
            "Step 664/1000, Loss: 0.045845434069633484, LR: 0.0001\n",
            "Step 665/1000, Loss: 0.04542561620473862, LR: 0.0001\n",
            "Step 666/1000, Loss: 0.043104227632284164, LR: 0.0001\n",
            "Step 667/1000, Loss: 0.042644113302230835, LR: 0.0001\n",
            "Step 668/1000, Loss: 0.04181262105703354, LR: 0.0001\n",
            "Step 669/1000, Loss: 0.04105354845523834, LR: 0.0001\n",
            "Step 670/1000, Loss: 0.040222905576229095, LR: 0.0001\n",
            "Step 671/1000, Loss: 0.03901106119155884, LR: 0.0001\n",
            "Step 672/1000, Loss: 0.037897102534770966, LR: 0.0001\n",
            "Step 673/1000, Loss: 0.03639138489961624, LR: 0.0001\n",
            "Step 674/1000, Loss: 0.035004161298274994, LR: 0.0001\n",
            "Step 675/1000, Loss: 0.03391029313206673, LR: 0.0001\n",
            "Step 676/1000, Loss: 0.03359318524599075, LR: 0.0001\n",
            "Step 677/1000, Loss: 0.03307536989450455, LR: 0.0001\n",
            "Step 678/1000, Loss: 0.03207049518823624, LR: 0.0001\n",
            "Step 679/1000, Loss: 0.03094908595085144, LR: 0.0001\n",
            "Step 680/1000, Loss: 0.030037332326173782, LR: 0.0001\n",
            "Step 681/1000, Loss: 0.02916393242776394, LR: 0.0001\n",
            "Step 682/1000, Loss: 0.02842871844768524, LR: 0.0001\n",
            "Step 683/1000, Loss: 0.027746105566620827, LR: 0.0001\n",
            "Step 684/1000, Loss: 0.026857439428567886, LR: 0.0001\n",
            "Step 685/1000, Loss: 0.026098018512129784, LR: 0.0001\n",
            "Step 686/1000, Loss: 0.025548327714204788, LR: 0.0001\n",
            "Step 687/1000, Loss: 0.024874145165085793, LR: 0.0001\n",
            "Step 688/1000, Loss: 0.02433035522699356, LR: 0.0001\n",
            "Step 689/1000, Loss: 0.023800110444426537, LR: 0.0001\n",
            "Step 690/1000, Loss: 0.023302342742681503, LR: 0.0001\n",
            "Step 691/1000, Loss: 0.02283920720219612, LR: 0.0001\n",
            "Step 692/1000, Loss: 0.02245541848242283, LR: 0.0001\n",
            "Step 693/1000, Loss: 0.021893080323934555, LR: 0.0001\n",
            "Step 694/1000, Loss: 0.02148192748427391, LR: 0.0001\n",
            "Step 695/1000, Loss: 0.021087493747472763, LR: 0.0001\n",
            "Step 696/1000, Loss: 0.02073487639427185, LR: 0.0001\n",
            "Step 697/1000, Loss: 0.020314160734415054, LR: 0.0001\n",
            "Step 698/1000, Loss: 0.019931694492697716, LR: 0.0001\n",
            "Step 699/1000, Loss: 0.019617551937699318, LR: 0.0001\n",
            "Step 700/1000, Loss: 0.019354116171598434, LR: 0.0001\n",
            "Step 701/1000, Loss: 0.019063036888837814, LR: 0.0001\n",
            "Step 702/1000, Loss: 0.018868882209062576, LR: 0.0001\n",
            "Step 703/1000, Loss: 0.018480753526091576, LR: 0.0001\n",
            "Step 704/1000, Loss: 0.018267787992954254, LR: 0.0001\n",
            "Step 705/1000, Loss: 0.01800377108156681, LR: 0.0001\n",
            "Step 706/1000, Loss: 0.01773737743496895, LR: 0.0001\n",
            "Step 707/1000, Loss: 0.017488760873675346, LR: 0.0001\n",
            "Step 708/1000, Loss: 0.017224617302417755, LR: 0.0001\n",
            "Step 709/1000, Loss: 0.01699564792215824, LR: 0.0001\n",
            "Step 710/1000, Loss: 0.016760531812906265, LR: 0.0001\n",
            "Step 711/1000, Loss: 0.016556452959775925, LR: 0.0001\n",
            "Step 712/1000, Loss: 0.016369324177503586, LR: 0.0001\n",
            "Step 713/1000, Loss: 0.016162054613232613, LR: 0.0001\n",
            "Step 714/1000, Loss: 0.015973437577486038, LR: 0.0001\n",
            "Step 715/1000, Loss: 0.015827396884560585, LR: 0.0001\n",
            "Step 716/1000, Loss: 0.01563403382897377, LR: 0.0001\n",
            "Step 717/1000, Loss: 0.015440216287970543, LR: 0.0001\n",
            "Step 718/1000, Loss: 0.015319311991333961, LR: 0.0001\n",
            "Step 719/1000, Loss: 0.015104414895176888, LR: 0.0001\n",
            "Step 720/1000, Loss: 0.014950106851756573, LR: 0.0001\n",
            "Step 721/1000, Loss: 0.014801112934947014, LR: 0.0001\n",
            "Step 722/1000, Loss: 0.014659660868346691, LR: 0.0001\n",
            "Step 723/1000, Loss: 0.01454732846468687, LR: 0.0001\n",
            "Step 724/1000, Loss: 0.014408242888748646, LR: 0.0001\n",
            "Step 725/1000, Loss: 0.014273661188781261, LR: 0.0001\n",
            "Step 726/1000, Loss: 0.014142356812953949, LR: 0.0001\n",
            "Step 727/1000, Loss: 0.014043526723980904, LR: 0.0001\n",
            "Step 728/1000, Loss: 0.013917172327637672, LR: 0.0001\n",
            "Step 729/1000, Loss: 0.013792688027024269, LR: 0.0001\n",
            "Step 730/1000, Loss: 0.013831773772835732, LR: 0.0001\n",
            "Step 731/1000, Loss: 0.013558072037994862, LR: 0.0001\n",
            "Step 732/1000, Loss: 0.01344936154782772, LR: 0.0001\n",
            "Step 733/1000, Loss: 0.013347037136554718, LR: 0.0001\n",
            "Step 734/1000, Loss: 0.013247156515717506, LR: 0.0001\n",
            "Step 735/1000, Loss: 0.01313665509223938, LR: 0.0001\n",
            "Step 736/1000, Loss: 0.013039764948189259, LR: 0.0001\n",
            "Step 737/1000, Loss: 0.012936721555888653, LR: 0.0001\n",
            "Step 738/1000, Loss: 0.012834340333938599, LR: 0.0001\n",
            "Step 739/1000, Loss: 0.012743964791297913, LR: 0.0001\n",
            "Step 740/1000, Loss: 0.01264624297618866, LR: 0.0001\n",
            "Step 741/1000, Loss: 0.01258203387260437, LR: 0.0001\n",
            "Step 742/1000, Loss: 0.01252621691673994, LR: 0.0001\n",
            "Step 743/1000, Loss: 0.01248594094067812, LR: 0.0001\n",
            "Step 744/1000, Loss: 0.012333648279309273, LR: 0.0001\n",
            "Step 745/1000, Loss: 0.012415305711328983, LR: 0.0001\n",
            "Step 746/1000, Loss: 0.012391500174999237, LR: 0.0001\n",
            "Step 747/1000, Loss: 0.012278889305889606, LR: 0.0001\n",
            "Step 748/1000, Loss: 0.012186388485133648, LR: 0.0001\n",
            "Step 749/1000, Loss: 0.012106169946491718, LR: 0.0001\n",
            "Step 750/1000, Loss: 0.012035012245178223, LR: 0.0001\n",
            "Step 751/1000, Loss: 0.011968955397605896, LR: 0.0001\n",
            "Step 752/1000, Loss: 0.011889743618667126, LR: 0.0001\n",
            "Step 753/1000, Loss: 0.011796589940786362, LR: 0.0001\n",
            "Step 754/1000, Loss: 0.01171401608735323, LR: 0.0001\n",
            "Step 755/1000, Loss: 0.011645877733826637, LR: 0.0001\n",
            "Step 756/1000, Loss: 0.011596804484724998, LR: 0.0001\n",
            "Step 757/1000, Loss: 0.011552165262401104, LR: 0.0001\n",
            "Step 758/1000, Loss: 0.011493055149912834, LR: 0.0001\n",
            "Step 759/1000, Loss: 0.011423677206039429, LR: 0.0001\n",
            "Step 760/1000, Loss: 0.011360431089997292, LR: 0.0001\n",
            "Step 761/1000, Loss: 0.011300052516162395, LR: 0.0001\n",
            "Step 762/1000, Loss: 0.01124151237308979, LR: 0.0001\n",
            "Step 763/1000, Loss: 0.011182071641087532, LR: 0.0001\n",
            "Step 764/1000, Loss: 0.011120229959487915, LR: 0.0001\n",
            "Step 765/1000, Loss: 0.011053986847400665, LR: 0.0001\n",
            "Step 766/1000, Loss: 0.010995345190167427, LR: 0.0001\n",
            "Step 767/1000, Loss: 0.010939346626400948, LR: 0.0001\n",
            "Step 768/1000, Loss: 0.010887585580348969, LR: 0.0001\n",
            "Step 769/1000, Loss: 0.010826846584677696, LR: 0.0001\n",
            "Step 770/1000, Loss: 0.010769437998533249, LR: 0.0001\n",
            "Step 771/1000, Loss: 0.01071271300315857, LR: 0.0001\n",
            "Step 772/1000, Loss: 0.010807265527546406, LR: 0.0001\n",
            "Step 773/1000, Loss: 0.010610172525048256, LR: 0.0001\n",
            "Step 774/1000, Loss: 0.010561642237007618, LR: 0.0001\n",
            "Step 775/1000, Loss: 0.010509582236409187, LR: 0.0001\n",
            "Step 776/1000, Loss: 0.010456422343850136, LR: 0.0001\n",
            "Step 777/1000, Loss: 0.01040586642920971, LR: 0.0001\n",
            "Step 778/1000, Loss: 0.010359649546444416, LR: 0.0001\n",
            "Step 779/1000, Loss: 0.010313289240002632, LR: 0.0001\n",
            "Step 780/1000, Loss: 0.010266265831887722, LR: 0.0001\n",
            "Step 781/1000, Loss: 0.010217590257525444, LR: 0.0001\n",
            "Step 782/1000, Loss: 0.01016851607710123, LR: 0.0001\n",
            "Step 783/1000, Loss: 0.010123779997229576, LR: 0.0001\n",
            "Step 784/1000, Loss: 0.010089741088449955, LR: 0.0001\n",
            "Step 785/1000, Loss: 0.010032610967755318, LR: 0.0001\n",
            "Step 786/1000, Loss: 0.009988253936171532, LR: 0.0001\n"
          ]
        }
      ],
      "source": [
        "# Example config:\n",
        "batch_size = 10\n",
        "sequence_len = 128\n",
        "num_steps = 1000\n",
        "train_inputs, train_targets, _, _ = get_dataset(10, sequence_len, 0)\n",
        "config = GPTConfig(\n",
        "    vocab_size=tokenizer.n_vocab,\n",
        "    n_layer=4,   # fewer layers for a quick demo\n",
        "    n_head=4,\n",
        "    n_embd=128,\n",
        "    seq_len=sequence_len,\n",
        ")\n",
        "\n",
        "\n",
        "# Create the GPT model\n",
        "model = GPTModel(config)\n",
        "\n",
        "# Define the optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=5e-4)\n",
        "\n",
        "# Define Scheduler\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min',factor=0.2, patience=20, min_lr=5e-6, threshold=1e-4)\n",
        "\n",
        "# Training loop\n",
        "i = 1\n",
        "losses = []\n",
        "\n",
        "while i < num_steps:\n",
        "    for j in range(0, len(train_inputs), batch_size):\n",
        "        x = train_inputs[j:j+batch_size]\n",
        "        y = train_targets[j:j+batch_size]\n",
        "\n",
        "        # Forward pass\n",
        "        logits = model(x)\n",
        "        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), y.view(-1))\n",
        "        loss.backward()\n",
        "\n",
        "        # Gradient clipping\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "        losses.append(loss.item())\n",
        "\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "\n",
        "        loss = loss.item()\n",
        "        scheduler.step(loss)\n",
        "\n",
        "\n",
        "        # Print the average loss for the epoch\n",
        "        lr = optimizer.param_groups[0][\"lr\"]\n",
        "        print(f\"Step {i+1}/{num_steps}, Loss: {loss}, LR: {lr}\")\n",
        "\n",
        "        i += 1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-mp4rrRPxKcB"
      },
      "outputs": [],
      "source": [
        "plt.plot(losses)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dSX_wx4RxKcB"
      },
      "source": [
        "### 1.3.8 Test Two: Memorization\n",
        "\n",
        "To perform inference, we can autoregressively feed data into the transformer, sliding the selected output token back into the input. We can test this on one of our training examples and see that our model is accurately reproducing the training example. The model has been overfit to the data, so we are testing if the model reproduces the correct outputs in the same order as the inputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kZcpy2Y8xKcB"
      },
      "outputs": [],
      "source": [
        "def inference(prompt, max_new_tokens):\n",
        "    tokens = tokenizer.encode(prompt)\n",
        "    for _ in range(max_new_tokens):\n",
        "        num_tokens = len(tokens)\n",
        "        tokens_padded = tokens + [tokenizer.eot_token] * (config.seq_len - num_tokens)\n",
        "        tokens_padded = torch.tensor(tokens_padded).unsqueeze(0).to(device)\n",
        "        logits = model(tokens_padded)\n",
        "        predicted_token = torch.argmax(logits[0, num_tokens-1, :]).item()\n",
        "        tokens.append(predicted_token)\n",
        "    return tokenizer.decode(tokens)\n",
        "\n",
        "print(\"Original: \", tokenizer.decode(train_inputs[2].tolist())[:90])\n",
        "print(\"Predicted:\", inference(\" director Takeshi Ozawa . A large team of writers handled the script\", max_new_tokens=6))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yYFVG4dOxKcC"
      },
      "source": [
        "## 1.4 Real Training Loop\n",
        "\n",
        "Using tiktoken, and a small dataset, we were able to overfit a small dataset and perform inference examples. However, in order to train a LLM that can do useful things we will need a larger dataset that won't be able to fit in memory. We will also need an efficient way to tokenize the dataset and load it into pytorch tensors.\n",
        "\n",
        "\n",
        "### 1.4.1 Huggingface Streaming Dataset\n",
        "Huggingface's datasets library makes this process very easy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rhTH-FKMxKcC"
      },
      "outputs": [],
      "source": [
        "# Load dataset in streaming mode\n",
        "ds = load_dataset(\"abisee/cnn_dailymail\", \"3.0.0\", split=\"train\")\n",
        "hf_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "def check_dataset_exists():\n",
        "    try:\n",
        "        # Attempt to load the dataset with reuse_cache_if_exists mode\n",
        "        load_dataset(\"parquet\", data_files=\"cnn_dailymail_train.parquet\", split=\"train\")\n",
        "        load_dataset(\"parquet\", data_files=\"cnn_dailymail_test.parquet\", split=\"train\")\n",
        "        return True\n",
        "    except FileNotFoundError:\n",
        "        return False\n",
        "\n",
        "if not check_dataset_exists():\n",
        "    print(\"Tokenized dataset does not exist locally... Generating and saving to disk.\")\n",
        "\n",
        "    def tokenize_and_chunk(dataset, tokenizer, chunk_size=512, train_rows=100_000, test_rows=500):\n",
        "        \"\"\"\n",
        "        Tokenizes and chunks the dataset into fixed-length 512-token segments.\n",
        "        The 'target' sequence is shifted left by 1 token.\n",
        "        Stops after generating `train_rows + test_rows` tokenized chunks.\n",
        "        \"\"\"\n",
        "        buffer = []  # Rolling buffer for tokens\n",
        "        row_count = 0\n",
        "\n",
        "        for example in dataset:\n",
        "            tokens = tokenizer(example[\"article\"], truncation=False, padding=False)['input_ids']\n",
        "            buffer.extend(tokens)\n",
        "\n",
        "            # Yield full chunks until we reach train_rows + test_rows\n",
        "            while len(buffer) >= chunk_size + 1:  # +1 to ensure we can shift target\n",
        "                if row_count >= (train_rows + test_rows):\n",
        "                    return  # Stop yielding once enough rows are reached\n",
        "\n",
        "                # Create input-target pairs\n",
        "                input_chunk = buffer[:chunk_size]         # First 512 tokens\n",
        "                target_chunk = buffer[1:chunk_size + 1]  # Shifted by 1 token\n",
        "\n",
        "                # Assign to train or test split\n",
        "                split = \"train\" if row_count < train_rows else \"test\"\n",
        "\n",
        "                yield {\n",
        "                    \"split\": split,\n",
        "                    \"input\": input_chunk,\n",
        "                    \"target\": target_chunk\n",
        "                }\n",
        "\n",
        "                buffer = buffer[chunk_size:]  # Remove used tokens\n",
        "                row_count += 1\n",
        "\n",
        "    # Set the max number of rows for training and testing\n",
        "    TRAIN_ROWS = 1400000  # Adjust as needed\n",
        "    TEST_ROWS = 500   # Adjust as needed\n",
        "    CHUNK_SIZE = 128\n",
        "\n",
        "    # Convert generator to a Hugging Face Dataset\n",
        "    tokenized_ds = Dataset.from_generator(lambda: tokenize_and_chunk(ds, hf_tokenizer,chunk_size=CHUNK_SIZE, train_rows=TRAIN_ROWS, test_rows=TEST_ROWS))\n",
        "\n",
        "    # Split the dataset into `train` and `test`\n",
        "    dataset_splits = tokenized_ds.train_test_split(test_size=TEST_ROWS / (TRAIN_ROWS + TEST_ROWS), seed=42)\n",
        "\n",
        "    # Save to disk\n",
        "    dataset_splits[\"train\"].to_parquet(\"cnn_dailymail_train.parquet\")\n",
        "    dataset_splits[\"test\"].to_parquet(\"cnn_dailymail_test.parquet\")\n",
        "\n",
        "    print(f\"✅ Saved {TRAIN_ROWS} train rows and {TEST_ROWS} test rows.\")\n",
        "else:\n",
        "    print(\"Tokenized dataset already exists locally.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DJYWVVZ8xKcC"
      },
      "source": [
        "\n",
        "### 1.4.2 Modified Training Loop\n",
        "We have tokenized the dataset in chunks, and saved it to the disk as a parquet file. This is a scalable approach that will allow us to train the model while never having the entire dataset in memory. Let's make a more robust training loop that ensures we are saving off the model at various checkpoints.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ayeMYIfxKcC"
      },
      "outputs": [],
      "source": [
        "# Example config:\n",
        "batch_size = 64\n",
        "sequence_len = 128\n",
        "num_steps = 150000\n",
        "accumulation_steps = 100\n",
        "\n",
        "\n",
        "# Reload the train and test datasets\n",
        "train_ds = load_dataset(\"parquet\", data_files=\"cnn_dailymail_train.parquet\", split=\"train\")\n",
        "test_ds = load_dataset(\"parquet\", data_files=\"cnn_dailymail_test.parquet\", split=\"train\")\n",
        "\n",
        "# Convert dataset to PyTorch format\n",
        "train_ds.set_format(\"torch\", columns=[\"input\", \"target\"])\n",
        "test_ds.set_format(\"torch\", columns=[\"input\", \"target\"])\n",
        "\n",
        "# Create DataLoaders for training and testing\n",
        "train_dataloader = cycle(DataLoader(train_ds, batch_size=batch_size, shuffle=False))\n",
        "test_dataloader = DataLoader(test_ds, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "config = GPTConfig(\n",
        "    vocab_size=hf_tokenizer.vocab_size,\n",
        "    n_layer=8,   # fewer layers for a quick demo\n",
        "    n_head=8,\n",
        "    n_embd=128,\n",
        "    seq_len=sequence_len,\n",
        ")\n",
        "\n",
        "# Create the GPT model\n",
        "model = GPTModel(config)\n",
        "\n",
        "use_existing_model = os.path.exists(\"./pretrain_final.pth\")\n",
        "# Check if pre-trained model exists\n",
        "if use_existing_model:\n",
        "    model = torch.load(\"./pretrain_final.pth\")\n",
        "    print(\"Loaded pre-trained model from ./pretrain_final.pth, skipping training loop.\")\n",
        "\n",
        "else:\n",
        "    # Define the optimizer\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=5e-4)\n",
        "\n",
        "\n",
        "    # Define Scheduler\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min',factor=0.3, patience=10, min_lr=5e-6, threshold=1e-4)\n",
        "\n",
        "\n",
        "    # Training loop\n",
        "    losses = []\n",
        "    test_losses = []\n",
        "    accumulator = 0\n",
        "    accumulator_loss = 0\n",
        "    start_time = time.time()\n",
        "    for i in range(num_steps):\n",
        "        model.train()\n",
        "        example = next(train_dataloader)\n",
        "        train_input = example[\"input\"].to(device)\n",
        "        train_target = example[\"target\"].to(device)\n",
        "\n",
        "        logits = model(train_input)\n",
        "        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), train_target.view(-1))\n",
        "        loss.backward()\n",
        "\n",
        "        # Gradient clipping\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "        # Update weights\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        accumulator += 1\n",
        "        accumulator_loss += loss.item()\n",
        "\n",
        "\n",
        "        if accumulator >= accumulation_steps:\n",
        "            losses.append(accumulator_loss / accumulation_steps)\n",
        "            accumulator = 0\n",
        "            accumulator_loss = 0\n",
        "            model.eval()\n",
        "            test_loss = 0\n",
        "            test_accumulator = 0\n",
        "            with torch.no_grad():\n",
        "                for test_example in test_dataloader:\n",
        "                    test_input = test_example[\"input\"].to(device)\n",
        "                    test_target = test_example[\"target\"].to(device)\n",
        "                    test_logits = model(test_input)\n",
        "                    test_loss += F.cross_entropy(test_logits.view(-1, test_logits.size(-1)), test_target.view(-1)).item()\n",
        "                    test_accumulator += 1\n",
        "                test_losses.append(test_loss / test_accumulator)\n",
        "                elapsed_time = time.time() - start_time\n",
        "                print(f\"Step {i+1}/{num_steps}, Loss: {losses[-1]}, Test Loss: {test_losses[-1]}, LR: {optimizer.param_groups[0]['lr']}, Elapsed Time: {elapsed_time:.2f} seconds\")\n",
        "                test_dataloader = DataLoader(test_ds, batch_size=batch_size, shuffle=False)\n",
        "                scheduler.step(test_losses[-1])\n",
        "\n",
        "\n",
        "        if (i+1) % 50000 == 0:\n",
        "            # Save the model checkpoint\n",
        "            print(f\"Saving model checkpoint at step {i+1}\")\n",
        "            torch.save(model, f\"./model_checkpoint_{i}.pt\")\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gzVszuHCxKcC"
      },
      "outputs": [],
      "source": [
        "if use_existing_model:\n",
        "    print(\"Existing model used, no loss curves shown.\")\n",
        "    plt.imshow(plt.imread(\"./loss_curve.png\"))\n",
        "else:\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(losses, label=\"Train Loss\", color='blue')\n",
        "    plt.plot(test_losses, label=\"Test Loss\", color='red')\n",
        "    plt.xlabel('Checkpoint')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Training and Test Loss Over Time')\n",
        "    plt.grid(True, linestyle='--', alpha=0.7)\n",
        "    plt.legend()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u44I-i5ExKcC"
      },
      "outputs": [],
      "source": [
        "if not use_existing_model:\n",
        "    torch.save(model, f\"./pretrain_final.pth\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vD6g2P6sxKcC"
      },
      "source": [
        "### 1.4.3 Inference with Pretrained Model\n",
        "\n",
        "Now that we have pretrained the model, we can perform some inference examples to see what types of outputs we get from the model. We can see that the model is able to output legible english, and most of the words make sense, however, its size limits make it not quite as robust as larger models. It is still good enough to see the \"sparks\" of understanding language.\n",
        "\n",
        "In this dataset, we trained on news articles so I've started the sentences with phrases that could potentially be found in the news. If you rerun the cell below this you will see that you get different outputs every time. This is due to the randomness of the next token selection step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VXC1s0_oxKcC"
      },
      "outputs": [],
      "source": [
        "def inference(prompt,torch_model, max_new_tokens):\n",
        "    torch_model.eval()\n",
        "    with torch.no_grad():\n",
        "        tokens = hf_tokenizer.encode(prompt)\n",
        "        for _ in range(max_new_tokens):\n",
        "            num_tokens = len(tokens)\n",
        "            tokens_padded = tokens + [hf_tokenizer.eos_token_id] * (config.seq_len - num_tokens)\n",
        "            tokens_padded = torch.tensor(tokens_padded).unsqueeze(0).to(device)\n",
        "            logits = torch_model(tokens_padded)\n",
        "            probabilities = torch.softmax(logits[0, num_tokens-1, :], dim=-1)\n",
        "            predicted_token = torch.multinomial(probabilities, 1).item()\n",
        "            tokens.append(predicted_token)\n",
        "        return hf_tokenizer.decode(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FmQeS_gsxKcC"
      },
      "outputs": [],
      "source": [
        "print(\"Predicted:\", inference(\"The president signed a bill to pass\", model, max_new_tokens=20))\n",
        "print(\"Predicted:\", inference(\"There was a large division in\", model, max_new_tokens=20))\n",
        "print(\"Predicted:\", inference(\"Reports are showing that\", model, max_new_tokens=20))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C3EDaYjWxKcC"
      },
      "source": [
        "# 2: Supervised Fine Tuning   \n",
        "To make the model more useable, we can take the pretrained model, and then go through a process called supervised fine tuning. This process involves having high quality supervised text datasets to get the model to respond how we want.\n",
        "\n",
        "We can use the [Fact Q&A](https://huggingface.co/datasets/rubenroy/GammaCorpus-Fact-QA-450k?library=datasets) dataset from huggingface for this. This dataset consists of question - answer examples that are short, which is good for our use case since we have a small context window of 128 tokens.\n",
        "\n",
        "Supervised fine tuning is where we can introduce \"tags\" and other types of text tokens that can help the model understand different roles in the text. For our dataset, we will have a \"question\" tag and an \"answer\" tag. We will add all of these when we create our dataset, and also during inference when a user submits a query. We also add eos tokens to end/pad the examples that do not take up the full context window.\n",
        "\n",
        "After fine tuning on this dataset, ideally we will have a LLM that you can ask a question and get an answer.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4zMxyU41xKcD"
      },
      "outputs": [],
      "source": [
        "# Load dataset in streaming mode\n",
        "sft_ds = load_dataset(\"rubenroy/GammaCorpus-Fact-QA-450k\", split=\"train\", streaming=True)\n",
        "\n",
        "def check_sft_dataset_exists():\n",
        "    try:\n",
        "        # Attempt to load the dataset with reuse_cache_if_exists mode\n",
        "        load_dataset(\"parquet\", data_files=\"fact_qa_train.parquet\", split=\"train\")\n",
        "        load_dataset(\"parquet\", data_files=\"fact_qa_test.parquet\", split=\"train\")\n",
        "        return True\n",
        "    except FileNotFoundError:\n",
        "        return False\n",
        "\n",
        "if not check_sft_dataset_exists():\n",
        "    print(\"Tokenized supervised fine tuning dataset does not exist locally... Generating and saving to disk.\")\n",
        "\n",
        "    def tokenize_and_chunk(dataset, tokenizer, chunk_size=512, rows=1000):\n",
        "        \"\"\"\n",
        "        Tokenizes and chunks the dataset into fixed-length 512-token segments.\n",
        "        The 'target' sequence is shifted left by 1 token.\n",
        "        Stops after generating `train_rows + test_rows` tokenized chunks.\n",
        "        \"\"\"\n",
        "        row_count = 0\n",
        "\n",
        "        for example in dataset:\n",
        "            question_plus_answer = \"<Question>\" + example[\"question\"] + \"</Question>\" + \"<Answer>\" + example[\"answer\"] + \"</Answer>\"\n",
        "            input_tokens = tokenizer(question_plus_answer, truncation=False, padding=False)['input_ids']\n",
        "\n",
        "            if row_count >= rows:\n",
        "                return\n",
        "\n",
        "            if len(input_tokens) >= chunk_size:\n",
        "                continue\n",
        "            else:\n",
        "                input_tokens = input_tokens +[tokenizer.eos_token_id] * (chunk_size - len(input_tokens))\n",
        "\n",
        "            target_tokens = input_tokens[1:] + [tokenizer.eos_token_id]  # Shifted by 1 token\n",
        "\n",
        "            yield {\n",
        "                \"input\": input_tokens,\n",
        "                \"target\": target_tokens\n",
        "            }\n",
        "\n",
        "            row_count += 1\n",
        "\n",
        "    # Set the max number of rows for training and testing\n",
        "    TRAIN_ROWS = 440000  # Adjust as needed\n",
        "    TEST_ROWS = 500   # Adjust as needed\n",
        "    CHUNK_SIZE = 128\n",
        "\n",
        "    # Convert generator to a Hugging Face Dataset\n",
        "    tokenized_sft_dataset = Dataset.from_generator(lambda: tokenize_and_chunk(sft_ds, hf_tokenizer,chunk_size=CHUNK_SIZE, rows=TRAIN_ROWS + TEST_ROWS))\n",
        "\n",
        "    # Split the dataset into `train` and `test`\n",
        "    sft_dataset_splits = tokenized_sft_dataset.train_test_split(train_size=TRAIN_ROWS, test_size=TEST_ROWS, seed=42)\n",
        "\n",
        "    # Save to disk\n",
        "    sft_dataset_splits[\"train\"].to_parquet(\"fact_qa_train.parquet\")\n",
        "    sft_dataset_splits[\"test\"].to_parquet(\"fact_qa_test.parquet\")\n",
        "\n",
        "    print(f\"✅ Saved {TRAIN_ROWS} train rows and {TEST_ROWS} test rows for supervised fine tuning.\")\n",
        "else:\n",
        "    print(\"SFT Tokenized dataset already exists locally.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Xi_38N5xKcD"
      },
      "source": [
        "### 2.1 Supervised Fine Tuning Training Loop\n",
        "\n",
        "A very similar training loop can be used for supervised fine tuning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RYgQyE5yxKcD"
      },
      "outputs": [],
      "source": [
        "# Example config:\n",
        "batch_size = 64\n",
        "sequence_len = 128\n",
        "num_steps = 50000\n",
        "accumulation_steps = 100\n",
        "\n",
        "# Reload the train and test datasets\n",
        "train_ds = load_dataset(\"parquet\", data_files=\"fact_qa_train.parquet\", split=\"train\")\n",
        "test_ds = load_dataset(\"parquet\", data_files=\"fact_qa_test.parquet\", split=\"train\")\n",
        "\n",
        "# Convert dataset to PyTorch format\n",
        "train_ds.set_format(\"torch\", columns=[\"input\", \"target\"])\n",
        "test_ds.set_format(\"torch\", columns=[\"input\", \"target\"])\n",
        "\n",
        "# Create DataLoaders for training and testing\n",
        "train_dataloader = cycle(DataLoader(train_ds, batch_size=batch_size, shuffle=False))\n",
        "test_dataloader = DataLoader(test_ds, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "\n",
        "use_existing_model = os.path.exists(\"./sft_final.pth\")\n",
        "# Check if pre-trained model exists\n",
        "if use_existing_model:\n",
        "    model = torch.load(\"./sft_final.pth\")\n",
        "    print(\"Loaded fine tuned model from ./sft_final.pth, skipping training loop.\")\n",
        "\n",
        "else:\n",
        "    # For SFT we start with the pretrained model\n",
        "    model = torch.load(\"./pretrain_final.pth\")\n",
        "\n",
        "    # Define the optimizer\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=5e-4)\n",
        "\n",
        "\n",
        "    # Scheduler with dynamic step size\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min',factor=0.2, patience=10, min_lr=5e-6, threshold=1e-4)\n",
        "\n",
        "\n",
        "    # Training loop\n",
        "    losses = []\n",
        "    test_losses = []\n",
        "    accumulator = 0\n",
        "    accumulator_loss = 0\n",
        "    for i in range(num_steps):\n",
        "        model.train()\n",
        "        example = next(train_dataloader)\n",
        "        train_input = example[\"input\"].to(device)\n",
        "        train_target = example[\"target\"].to(device)\n",
        "        logits = model(train_input)\n",
        "        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), train_target.view(-1))\n",
        "        loss.backward()\n",
        "\n",
        "        # Gradient clipping\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "        # Update weights\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        accumulator += 1\n",
        "        accumulator_loss += loss.item()\n",
        "\n",
        "\n",
        "\n",
        "        if accumulator >= accumulation_steps:\n",
        "            losses.append(accumulator_loss / accumulation_steps)\n",
        "            accumulator = 0\n",
        "            accumulator_loss = 0\n",
        "            model.eval()\n",
        "            test_loss = 0\n",
        "            test_accumulator = 0\n",
        "            with torch.no_grad():\n",
        "                for test_example in test_dataloader:\n",
        "                    test_input = test_example[\"input\"].to(device)\n",
        "                    test_target = test_example[\"target\"].to(device)\n",
        "                    test_logits = model(test_input)\n",
        "                    test_loss += F.cross_entropy(test_logits.view(-1, test_logits.size(-1)), test_target.view(-1)).item()\n",
        "                    test_accumulator += 1\n",
        "                test_losses.append(test_loss / test_accumulator)\n",
        "                print(f\"Step {i+1}/{num_steps}, Loss: {losses[-1]}, Test Loss: {test_losses[-1]}\")\n",
        "                test_dataloader = DataLoader(test_ds, batch_size=batch_size, shuffle=False)\n",
        "                scheduler.step(test_losses[-1])\n",
        "\n",
        "        if i+1 % 50000 == 0:\n",
        "            torch.save(model.state_dict(), f\"./sft_model_checkpoint_{i}.pt\")\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ioOYXJSpxKcD"
      },
      "outputs": [],
      "source": [
        "if use_existing_model:\n",
        "    print(\"Existing model used, no loss curves shown.\")\n",
        "    plt.imshow(plt.imread(\"./sft_loss_curve.png\"))\n",
        "else:\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(losses, label=\"Train Loss\", color='blue')\n",
        "    plt.plot(test_losses, label=\"Test Loss\", color='red')\n",
        "    plt.xlabel('Checkpoint')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Supervised Fine Tuning - Training and Test Loss Over Time')\n",
        "    plt.grid(True, linestyle='--', alpha=0.7)\n",
        "    plt.legend()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IZFGVJ5fxKcD"
      },
      "outputs": [],
      "source": [
        "if not use_existing_model:\n",
        "    torch.save(model, f\"./sft_final.pth\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PfBwEEUQxKcD"
      },
      "source": [
        "### 2.2 Inference with Fine Tuned Model\n",
        "\n",
        "With the fine tuned model, we can perform a more natural form of interence. Instead of formatting all of our prompts as next token prediction, we can have a more natural Q&A style format with the model    \n",
        "\n",
        "We are using a very small model and a very small set of data compared to modern LLMs, so our model is not going to perform very well on most questions. However, it is outputting responses that are at least related to the prompt and are formatted in a correct way. It is very cool to see the LLM starting to come together! As we scale up the model, data, etc... the responses will become more factual, realistic, and contextually accurate. At this point, the majority of the responses are hallucinations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8AiFnOYDxKcD"
      },
      "outputs": [],
      "source": [
        "def sft_inference(prompt,torch_model, max_new_tokens):\n",
        "    torch_model.eval()\n",
        "    prompt = \"<Question>\" + prompt + \"</Question>\" + \"<Answer>\" # Wrap the prompt in <Question> and start inference with <Answer>\n",
        "    with torch.no_grad():\n",
        "        tokens = hf_tokenizer.encode(prompt) # Tokenize the prompt\n",
        "        for _ in range(max_new_tokens):\n",
        "            if tokens[-1] == hf_tokenizer.eos_token_id: # Stop if we reach the end of the sequence\n",
        "                break\n",
        "            num_tokens = len(tokens) #\n",
        "            tokens_padded = tokens + [hf_tokenizer.eos_token_id] * (config.seq_len - num_tokens) # pad the sequence with eos token\n",
        "            tokens_padded = torch.tensor(tokens_padded).unsqueeze(0).to(device)\n",
        "            logits = torch_model(tokens_padded) # Forward pass through the model\n",
        "            probabilities = torch.softmax(logits[0, num_tokens-1, :], dim=-1) # Get the probabilities of the last token\n",
        "            predicted_token = torch.argmax(probabilities).item() # Greedy decoding, change to sampling for more diversity\n",
        "            tokens.append(predicted_token)\n",
        "\n",
        "        # Strip the text to between the <Answer></Answer> tags\n",
        "        full_answer = hf_tokenizer.decode(tokens)\n",
        "        answer_start = full_answer.find(\"<Answer>\") + len(\"<Answer>\")\n",
        "        answer_end = full_answer.find(\"</Answer>\")\n",
        "        return full_answer[answer_start:answer_end]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0pE5x8sHxKcD"
      },
      "outputs": [],
      "source": [
        "print(\"Predicted:\", sft_inference(\"Who is the most powerful leader in the west?\", model, max_new_tokens=20))\n",
        "print(\"Predicted:\", sft_inference(\"What color is the sun?\", model, max_new_tokens=20))\n",
        "print(\"Predicted:\", sft_inference(\"What color is the ocean\", model, max_new_tokens=20))\n",
        "print(\"Predicted:\", sft_inference(\"How many planets are in the solar system\", model, max_new_tokens=20))\n",
        "print(\"Predicted:\", sft_inference(\"What three countries are in north america?\", model, max_new_tokens=20))\n",
        "print(\"Predicted:\", sft_inference(\"How many eyes do humans have?\", model, max_new_tokens=20))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NCnQeODpxKcD"
      },
      "source": [
        "# Sources\n",
        "- [1] [Improving Language Understanding by Generative Pretraining (GPT)](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)\n",
        "- [2] [Pytorch](https://pytorch.org/)\n",
        "- [3] [Language Models are Unsupervised Multitask Learners (GPT2)](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)\n",
        "- [4] [Language Models are Few-Shot Learners (GPT3)](https://arxiv.org/pdf/2005.14165)\n",
        "- [5] [Tokenization](https://nlp.stanford.edu/IR-book/html/htmledition/tokenization-1.html)\n",
        "- [6] [Salesforce Wikitext](https://huggingface.co/datasets/EleutherAI/wikitext_document_level)\n",
        "- [7] [Huggingface Datasets](https://huggingface.co/docs/datasets/en/index)\n",
        "- [8] [Tiktoken](https://github.com/openai/tiktoken)\n",
        "- [9] [Attention is All You Need](https://arxiv.org/pdf/1706.03762)\n",
        "- [10] [Gentle Introduction to Positional Encoding](https://machinelearningmastery.com/a-gentle-introduction-to-positional-encoding-in-transformer-models-part-1/)\n",
        "- [11] [How Do Self-Attention Masks Work?](https://gmongaras.medium.com/how-do-self-attention-masks-work-72ed9382510f)\n",
        "- [12] [CrossEntropyLoss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html)\n",
        "- [13] [Word2Vec](https://arxiv.org/pdf/1301.3781)\n",
        "- [14] [nanoGPT](https://github.com/karpathy/nanoGPT)\n",
        "- [15] [3blue1brown: Attention in Transformers](https://www.youtube.com/watch?v=eMlx5fFNoYc&t=1s)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "676f0a9df47e4b1780a1719ee9a36799": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2b1f827611334219a87b2bf4eeb125fa",
              "IPY_MODEL_5c82932ed45541fa8874a155d7ef8bc0",
              "IPY_MODEL_acada8fa12fc490c8144cb58e5eff5e5"
            ],
            "layout": "IPY_MODEL_7608de0d7776416da8ead1a6571f1d81"
          }
        },
        "2b1f827611334219a87b2bf4eeb125fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f5ddd2be42a04a4ca081d6d7e7f7e0aa",
            "placeholder": "​",
            "style": "IPY_MODEL_78dd831a2b1e4771b231c3b3ce9bb8b0",
            "value": "README.md: "
          }
        },
        "5c82932ed45541fa8874a155d7ef8bc0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dfd17a3ba21342c5a80af27c6b02f014",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_dd7289f68b2641f0a3819c85028bb579",
            "value": 1
          }
        },
        "acada8fa12fc490c8144cb58e5eff5e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_86f8fbea683b4fa196af95f94e475ed1",
            "placeholder": "​",
            "style": "IPY_MODEL_6daa4bc31aa24c6c822a93d85421dad9",
            "value": " 8.76k/? [00:00&lt;00:00, 614kB/s]"
          }
        },
        "7608de0d7776416da8ead1a6571f1d81": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f5ddd2be42a04a4ca081d6d7e7f7e0aa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "78dd831a2b1e4771b231c3b3ce9bb8b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dfd17a3ba21342c5a80af27c6b02f014": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "dd7289f68b2641f0a3819c85028bb579": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "86f8fbea683b4fa196af95f94e475ed1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6daa4bc31aa24c6c822a93d85421dad9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "54f8bbdf3f1340d58ce36198fb0f571e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9c731d95857a4a73a571904f752561da",
              "IPY_MODEL_c655b5dc24614a60b92cff86547aa011",
              "IPY_MODEL_489656a9fbb04a528dd581792483fae9"
            ],
            "layout": "IPY_MODEL_f1fe334d21b84df9a69fe61c110376be"
          }
        },
        "9c731d95857a4a73a571904f752561da": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_badf75616ce0469496bb8b484a90ef81",
            "placeholder": "​",
            "style": "IPY_MODEL_36648846a61c426ba7b214b0cf19df11",
            "value": "wikitext-2-raw-v1/wikitext-2-raw-v1-trai(…): 100%"
          }
        },
        "c655b5dc24614a60b92cff86547aa011": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_17c3de396aa840628078ff3beaf8b152",
            "max": 6179128,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_abe31d274e7141f7b83a9ba9d3b798ca",
            "value": 6179128
          }
        },
        "489656a9fbb04a528dd581792483fae9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_474449ab477740deb0bdafb78550bfde",
            "placeholder": "​",
            "style": "IPY_MODEL_ca4887bd7aaa43db92e1e4bd2ecd5b1a",
            "value": " 6.18M/6.18M [00:00&lt;00:00, 7.52MB/s]"
          }
        },
        "f1fe334d21b84df9a69fe61c110376be": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "badf75616ce0469496bb8b484a90ef81": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "36648846a61c426ba7b214b0cf19df11": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "17c3de396aa840628078ff3beaf8b152": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "abe31d274e7141f7b83a9ba9d3b798ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "474449ab477740deb0bdafb78550bfde": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ca4887bd7aaa43db92e1e4bd2ecd5b1a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dd7411cdd0fe46e9937b4bad0a38ee00": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_90d253e05204480ca5ecfc28b31efcc1",
              "IPY_MODEL_522b5fba838a4fa89e87b7fb9b33755c",
              "IPY_MODEL_6e1bd04e8cf14915bb84707a45931631"
            ],
            "layout": "IPY_MODEL_d1265802ef7e4b0aaf1c33802e38f4b0"
          }
        },
        "90d253e05204480ca5ecfc28b31efcc1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8c2bb652770e4b9d9005a1bbb114c045",
            "placeholder": "​",
            "style": "IPY_MODEL_2d14eb1733784154844b120585c4f8cc",
            "value": "wikitext-2-raw-v1/wikitext-2-raw-v1-vali(…): 100%"
          }
        },
        "522b5fba838a4fa89e87b7fb9b33755c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_240c2a477007430fa86ca2a9cbdc90e7",
            "max": 640769,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c42a0a3eee244624b1eb18307c4e6239",
            "value": 640769
          }
        },
        "6e1bd04e8cf14915bb84707a45931631": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fa84b09d70024f9cab9befbc7ef690c3",
            "placeholder": "​",
            "style": "IPY_MODEL_bf9e86f673454ba088f0b7bdccb8006b",
            "value": " 641k/641k [00:00&lt;00:00, 1.15MB/s]"
          }
        },
        "d1265802ef7e4b0aaf1c33802e38f4b0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8c2bb652770e4b9d9005a1bbb114c045": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2d14eb1733784154844b120585c4f8cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "240c2a477007430fa86ca2a9cbdc90e7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c42a0a3eee244624b1eb18307c4e6239": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fa84b09d70024f9cab9befbc7ef690c3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bf9e86f673454ba088f0b7bdccb8006b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "364dd88fa9304a28900876206f3dd279": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_751fd407ed8643a89f067f5d3c8320bd",
              "IPY_MODEL_48334931bf9142f089ae44714a1c6ec8",
              "IPY_MODEL_6270c3c069c74627be1ed565c529ebef"
            ],
            "layout": "IPY_MODEL_83cca88897474b98800a582060c60515"
          }
        },
        "751fd407ed8643a89f067f5d3c8320bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5eee864a2963411b861119fb86c48df0",
            "placeholder": "​",
            "style": "IPY_MODEL_32db1c53ae7f4f81864f6dc26ec94039",
            "value": "wikitext-2-raw-v1/wikitext-2-raw-v1-test(…): 100%"
          }
        },
        "48334931bf9142f089ae44714a1c6ec8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_411ae67af1ba46a4ad6966152a596408",
            "max": 715204,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7148ca36f85648969134e2960885ef59",
            "value": 715204
          }
        },
        "6270c3c069c74627be1ed565c529ebef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ac4d02661d8d444eb5e4495046307bf5",
            "placeholder": "​",
            "style": "IPY_MODEL_6e017ce6e7534b92b1ce79906c9a4973",
            "value": " 715k/715k [00:00&lt;00:00, 1.83MB/s]"
          }
        },
        "83cca88897474b98800a582060c60515": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5eee864a2963411b861119fb86c48df0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "32db1c53ae7f4f81864f6dc26ec94039": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "411ae67af1ba46a4ad6966152a596408": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7148ca36f85648969134e2960885ef59": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ac4d02661d8d444eb5e4495046307bf5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6e017ce6e7534b92b1ce79906c9a4973": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "49a97eeb72a64afe9e7dbbb9190ee0f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_aec33ef6544b447fb2fdaf31dbc62250",
              "IPY_MODEL_cd3b7378291b48338ee6104aea96e775",
              "IPY_MODEL_32f29fc62c444b7fb35e35d749fcd297"
            ],
            "layout": "IPY_MODEL_6a4d2d81c2d64092b71a94c21be271e3"
          }
        },
        "aec33ef6544b447fb2fdaf31dbc62250": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c1f5c05bcfd140899fed3461b32450ae",
            "placeholder": "​",
            "style": "IPY_MODEL_25feec3f13224bd4b3e84d753a2cabe4",
            "value": "Generating train split: 100%"
          }
        },
        "cd3b7378291b48338ee6104aea96e775": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f5e97813d236455daea8e2295d9cbe7c",
            "max": 629,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7bb7bfb6d04e4af5b8668634f26df806",
            "value": 629
          }
        },
        "32f29fc62c444b7fb35e35d749fcd297": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6f9acb0deafc42a2a90730db002cb884",
            "placeholder": "​",
            "style": "IPY_MODEL_eb071085bafa49f68d542f811fe2f360",
            "value": " 629/629 [00:00&lt;00:00, 3194.32 examples/s]"
          }
        },
        "6a4d2d81c2d64092b71a94c21be271e3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c1f5c05bcfd140899fed3461b32450ae": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "25feec3f13224bd4b3e84d753a2cabe4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f5e97813d236455daea8e2295d9cbe7c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7bb7bfb6d04e4af5b8668634f26df806": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6f9acb0deafc42a2a90730db002cb884": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eb071085bafa49f68d542f811fe2f360": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6a2f0255c0c74a7b9ad90853d1f5aa2f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_64060c0957314d8184fde547c2f7674c",
              "IPY_MODEL_fe2b9d1b64e44e5295e810c221fc9b79",
              "IPY_MODEL_efa4c0c93fd54c8783c43d0453175acf"
            ],
            "layout": "IPY_MODEL_575b0ed3c76b428f94ad1354168cf71b"
          }
        },
        "64060c0957314d8184fde547c2f7674c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5d1f07008db74fdcb1c9c07dc0187254",
            "placeholder": "​",
            "style": "IPY_MODEL_a623c7f65a2e4bf499f5b77960c399d4",
            "value": "Generating validation split: 100%"
          }
        },
        "fe2b9d1b64e44e5295e810c221fc9b79": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5269ba94a10d40bcb00cfba2a9a28667",
            "max": 60,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_16326b01882447ada08b093974d7c069",
            "value": 60
          }
        },
        "efa4c0c93fd54c8783c43d0453175acf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c925a75f9a1e4e3a8859c07ed934442b",
            "placeholder": "​",
            "style": "IPY_MODEL_9d94fa39c5f64734a9f45ca909e4ec52",
            "value": " 60/60 [00:00&lt;00:00, 1181.58 examples/s]"
          }
        },
        "575b0ed3c76b428f94ad1354168cf71b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5d1f07008db74fdcb1c9c07dc0187254": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a623c7f65a2e4bf499f5b77960c399d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5269ba94a10d40bcb00cfba2a9a28667": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "16326b01882447ada08b093974d7c069": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c925a75f9a1e4e3a8859c07ed934442b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9d94fa39c5f64734a9f45ca909e4ec52": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "df3c8500ad7f4468815648693de092d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_461e81974b4442f8a10f79590f6b7eb7",
              "IPY_MODEL_22733d11590743d5b751c8b828b9a879",
              "IPY_MODEL_60f4c2ff247d499d9018cd231c7dc7c3"
            ],
            "layout": "IPY_MODEL_21e1a2d783734364b1bff3c736f1e115"
          }
        },
        "461e81974b4442f8a10f79590f6b7eb7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b8db1efa578f478e8d85229266b12bfa",
            "placeholder": "​",
            "style": "IPY_MODEL_e0d8c50e8e9149ae8fb17a106ac9d4ff",
            "value": "Generating test split: 100%"
          }
        },
        "22733d11590743d5b751c8b828b9a879": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_58ab936e783147659591d07a51dd993b",
            "max": 62,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_44bedb1cfdc04a6fa95d0802cf223b0c",
            "value": 62
          }
        },
        "60f4c2ff247d499d9018cd231c7dc7c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fb64a20bc4e14f9a8150f136ce87ee75",
            "placeholder": "​",
            "style": "IPY_MODEL_d2c981a66ac241aeafee87639a769b6f",
            "value": " 62/62 [00:00&lt;00:00, 1142.16 examples/s]"
          }
        },
        "21e1a2d783734364b1bff3c736f1e115": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b8db1efa578f478e8d85229266b12bfa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e0d8c50e8e9149ae8fb17a106ac9d4ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "58ab936e783147659591d07a51dd993b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "44bedb1cfdc04a6fa95d0802cf223b0c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fb64a20bc4e14f9a8150f136ce87ee75": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d2c981a66ac241aeafee87639a769b6f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}